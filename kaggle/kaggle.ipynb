{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033f89d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:14.960735Z",
     "iopub.status.busy": "2024-11-18T06:41:14.960244Z",
     "iopub.status.idle": "2024-11-18T06:41:20.261668Z",
     "shell.execute_reply": "2024-11-18T06:41:20.260678Z",
     "shell.execute_reply.started": "2024-11-18T06:41:14.960677Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54085196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:20.263860Z",
     "iopub.status.busy": "2024-11-18T06:41:20.263404Z",
     "iopub.status.idle": "2024-11-18T06:41:20.268526Z",
     "shell.execute_reply": "2024-11-18T06:41:20.267631Z",
     "shell.execute_reply.started": "2024-11-18T06:41:20.263826Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a734b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:20.270159Z",
     "iopub.status.busy": "2024-11-18T06:41:20.269803Z",
     "iopub.status.idle": "2024-11-18T06:41:20.319247Z",
     "shell.execute_reply": "2024-11-18T06:41:20.318177Z",
     "shell.execute_reply.started": "2024-11-18T06:41:20.270127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA as device\n"
     ]
    }
   ],
   "source": [
    "# define device \n",
    "# configuration \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e657754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:20.322433Z",
     "iopub.status.busy": "2024-11-18T06:41:20.321470Z",
     "iopub.status.idle": "2024-11-18T06:41:20.326536Z",
     "shell.execute_reply": "2024-11-18T06:41:20.325537Z",
     "shell.execute_reply.started": "2024-11-18T06:41:20.322397Z"
    }
   },
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "preprocessed_directory = preprocessed_directory = os.path.join(current_path, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18df9f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:20.328473Z",
     "iopub.status.busy": "2024-11-18T06:41:20.327826Z",
     "iopub.status.idle": "2024-11-18T06:41:25.780125Z",
     "shell.execute_reply": "2024-11-18T06:41:25.779248Z",
     "shell.execute_reply.started": "2024-11-18T06:41:20.328440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e225c44cbf45ccb0deeb465b205ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6011368f2d347668f8c85ce6e11b614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/344k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2394b90119e2431bba72214b22ddecb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc31224e644b42dcb17fdf200e019636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7579c470724f05b455e13701bed7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07769fde235143a78a95ef0e4d4261d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6f641355f24fe3be57187a36d50b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import tokenizers\n",
    "kr_tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "en_tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183a89d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:25.781761Z",
     "iopub.status.busy": "2024-11-18T06:41:25.781363Z",
     "iopub.status.idle": "2024-11-18T06:41:25.789892Z",
     "shell.execute_reply": "2024-11-18T06:41:25.789121Z",
     "shell.execute_reply.started": "2024-11-18T06:41:25.781709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test tokenizers \n",
    "tmp_kr_sentence = \"오늘 하교길에 길고양이를 보았는데, 너무 귀여워서 집에 데려가고 싶었다. 하지만 그러지는 않았다.\"\n",
    "tmp_en_sentence = \"The cat I saw during heading home today was so cute, that I wanted to bring it to home.\"\n",
    "\n",
    "tmp_kr_tokenized = kr_tokenizer(tmp_kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "tmp_en_tokenized = en_tokenizer(tmp_en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "\n",
    "# print(kr_tokenizer.convert_ids_to_tokens(tmp_kr_tokenized.input_ids))\n",
    "# print(en_tokenizer.convert_ids_to_tokens(tmp_en_tokenized.input_ids))\n",
    "\n",
    "# print(kr_tokenizer.decode(tmp_kr_tokenized.input_ids, skip_special_tokens=True))\n",
    "\n",
    "# check if both tokenizer has pad token \n",
    "# print(kr_tokenizer.pad_token)\n",
    "# print(en_tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05c2b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:25.791846Z",
     "iopub.status.busy": "2024-11-18T06:41:25.791241Z",
     "iopub.status.idle": "2024-11-18T06:41:30.894710Z",
     "shell.execute_reply": "2024-11-18T06:41:30.893852Z",
     "shell.execute_reply.started": "2024-11-18T06:41:25.791804Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(path=\"/kaggle/input/train.parquet\")\n",
    "df_test = pd.read_parquet(path=\"/kaggle/input/test.parquet\")\n",
    "df_validation = pd.read_parquet(path=\"/kaggle/input/validation.parquet\")\n",
    "\n",
    "class en2kr_Train_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_train\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "\n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        return kr_tokenized_ids, en_tokenized_ids\n",
    "        \n",
    "class en2kr_Test_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_test\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "        \n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        \n",
    "        return kr_tokenized_ids, en_tokenized_ids\n",
    "\n",
    "class en2kr_Validation_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_validation\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "        \n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        \n",
    "        return kr_tokenized_ids, en_tokenized_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc3662b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:30.896164Z",
     "iopub.status.busy": "2024-11-18T06:41:30.895858Z",
     "iopub.status.idle": "2024-11-18T06:41:30.902921Z",
     "shell.execute_reply": "2024-11-18T06:41:30.901878Z",
     "shell.execute_reply.started": "2024-11-18T06:41:30.896132Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = en2kr_Train_Dataset(max_len=128)\n",
    "test_dataset = en2kr_Test_Dataset(max_len=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True,  shuffle=True, generator=torch.Generator(device=device), pin_memory=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, generator=torch.Generator(device=device), pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623e164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39406dd5",
   "metadata": {},
   "source": [
    "# Transformer Model Implementation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1f5306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:30.904542Z",
     "iopub.status.busy": "2024-11-18T06:41:30.904144Z",
     "iopub.status.idle": "2024-11-18T06:41:34.074812Z",
     "shell.execute_reply": "2024-11-18T06:41:34.073800Z",
     "shell.execute_reply.started": "2024-11-18T06:41:30.904480Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import copy \n",
    "import math \n",
    "from torch.nn.functional import log_softmax\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f3ffb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.080002Z",
     "iopub.status.busy": "2024-11-18T06:41:34.079111Z",
     "iopub.status.idle": "2024-11-18T06:41:34.094722Z",
     "shell.execute_reply": "2024-11-18T06:41:34.093735Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.079963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a Token Embedding \n",
    "class TokenEmbeddings(nn.Embedding): \n",
    "    \"\"\"\n",
    "    Converting token into embedding vector\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        class for token embedding without positional encoding\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super(TokenEmbeddings, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "\n",
    "# Define Positional Encoding \n",
    "class PositionalEncoding(nn.Module): \n",
    "    \"\"\" \n",
    "    compute reusable sinusoid positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len, device): \n",
    "        \"\"\"\n",
    "        construct sinusoid positional encoding that is going to be reused everytime when it is needed\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # define a max_len * d_model size encoding matrix\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "\n",
    "        # since positional encoding is not learnable, we turn off the gradient engine\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        # define a position at the sequence\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        # expand the max_len vector to max_len * 1 matrix \n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        # define a sinusoid positional encoding\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "# Define Transformer Embedding \n",
    "class TransformerEmbedding(nn.Module): \n",
    "    \"\"\"\n",
    "    token embedding + positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device): \n",
    "        \"\"\"\n",
    "        initialize the embedding class for word+position embedding\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        :param drop_prob: dropout probability to reduce overfitting\n",
    "        \"\"\"\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_emb = TokenEmbeddings(vocab_size, d_model)\n",
    "        self.position_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        tok_emb = self.token_emb(x)\n",
    "        pos_emb = self.position_emb(x)\n",
    "\n",
    "        return self.dropout(tok_emb+pos_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e49d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.096382Z",
     "iopub.status.busy": "2024-11-18T06:41:34.095982Z",
     "iopub.status.idle": "2024-11-18T06:41:34.111521Z",
     "shell.execute_reply": "2024-11-18T06:41:34.110704Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.096326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Attention Block \n",
    "class AttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    compute scale dot product attention for Query, Key, Value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, eps=1e-12): \n",
    "        batch_size, head, length, d_tensor = k.shape\n",
    "\n",
    "        # calculate the k_T\n",
    "        k_T = k.transpose(2, 3)\n",
    "\n",
    "        # calculate the attention weight \n",
    "        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n",
    "\n",
    "        # if there are any masks that needs to be applied\n",
    "        if mask is not None:\n",
    "            att_weight = att_weight.masked_fill(mask == 0, -10000)\n",
    "\n",
    "        # calculate the softmax \n",
    "        # att_weight shape: batch_size, head, seq_len, seq_len\n",
    "        att_weight = self.softmax(att_weight)\n",
    "\n",
    "        # att_weight @ v shape: batch_size, head, seq_len, d_model\n",
    "        return att_weight @ v, att_weight\n",
    "\n",
    "# Define MultiHeadAttention Block \n",
    "class MultiHeadAttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    define multi head attention block using AttentionBlock module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_head): \n",
    "        \"\"\"\n",
    "        Multi-head self-attention utilize the parallelism of GPU\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param n_head: number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = AttentionBlock()\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # in the paper, d_v = d_k = d_q\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.Wconcat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split(self, tensor): \n",
    "        \"\"\"\n",
    "        split the tensor by number of head \n",
    "\n",
    "        :param tensor: tensor of shape batch_size  * seq_len * d_model\n",
    "        :return: return tensor of shape batch_size * n_head * seq_len * d_tensor\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = tensor.shape \n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "\n",
    "        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor \n",
    "\n",
    "    def concat(self, tensor): \n",
    "        \"\"\"\n",
    "        concat tensor. Inverse operation of split\n",
    "\n",
    "        :param tensor: tensor of shape batch_size * n_head * seq_len * d_tensor \n",
    "        :return: return tensor of shape batch_size * seq_len * d_model\n",
    "        \"\"\"\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.shape\n",
    "\n",
    "        d_model = n_head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return tensor \n",
    "    \n",
    "\n",
    "    def forward(self, q, k, v, mask=None): \n",
    "        # apply linear transformation to derive q, k, v \n",
    "        q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)\n",
    "\n",
    "        # split the tensor by number of heads\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        # apply attention to q, k, v \n",
    "        out, att_weight = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # concat \n",
    "        out = self.concat(out)\n",
    "\n",
    "        # apply concat weight \n",
    "        out = self.Wconcat(out)\n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7fa7e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.113347Z",
     "iopub.status.busy": "2024-11-18T06:41:34.112721Z",
     "iopub.status.idle": "2024-11-18T06:41:34.122368Z",
     "shell.execute_reply": "2024-11-18T06:41:34.121499Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.113313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define LayerNorm \n",
    "class LayerNorm(nn.Module): \n",
    "    \"\"\"\n",
    "    Normalize all features for each samples. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-12): \n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, x): \n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "686c96d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.132560Z",
     "iopub.status.busy": "2024-11-18T06:41:34.132203Z",
     "iopub.status.idle": "2024-11-18T06:41:34.140478Z",
     "shell.execute_reply": "2024-11-18T06:41:34.139583Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.132525Z"
    }
   },
   "outputs": [],
   "source": [
    "# define FeedForward Network \n",
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1): \n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden) \n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9091c31c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.142045Z",
     "iopub.status.busy": "2024-11-18T06:41:34.141733Z",
     "iopub.status.idle": "2024-11-18T06:41:34.155722Z",
     "shell.execute_reply": "2024-11-18T06:41:34.154899Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.142012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Encoder Layer \n",
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm1 = LayerNorm(d_model) \n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask): \n",
    "        residual = x \n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "\n",
    "        x = self.dropout1(x) \n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x) \n",
    "\n",
    "        x =  self.dropout2(x)\n",
    "        x = self.norm2(x + residual)\n",
    "\n",
    "        return x \n",
    "\n",
    "# Define Decoder Layer \n",
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask): \n",
    "        residual = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        if enc is not None: \n",
    "            residual = x \n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "            x = self.dropout2(x) \n",
    "            x = self.norm2(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + residual)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef60e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.157249Z",
     "iopub.status.busy": "2024-11-18T06:41:34.156979Z",
     "iopub.status.idle": "2024-11-18T06:41:34.176937Z",
     "shell.execute_reply": "2024-11-18T06:41:34.176161Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.157220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Encoder Model\n",
    "class Encoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=enc_voc_size, drop_prob=drop_prob, device=device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask): \n",
    "        x = self.emb(x) \n",
    "        for layer in self.layers: \n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "        \n",
    "class Decoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=dec_voc_size, drop_prob=drop_prob, device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask): \n",
    "        trg = self.emb(trg)\n",
    "        for layer in self.layers: \n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask) \n",
    "\n",
    "        output = self.linear(trg)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Define Transformer Model \n",
    "class Transformer(nn.Module): \n",
    "    \"\"\"\n",
    "    Transformer Model\n",
    "    \"\"\"\n",
    "    def __init__(self, src_pad_token, trg_pad_token, trg_sos_token, enc_voc_size, dec_voc_size, n_head, max_len, ffn_hidden, n_layers, drop_prob, device): \n",
    "        \"\"\"\n",
    "        Constructing Transformer Model \n",
    "\n",
    "        :param src_pad_token: embedding vector that represents <pad> in source \n",
    "        :param trg_pad_token: embedding vector that represents <pad> in target \n",
    "        :param trg_sos_token: embedding vector that represents <sos> in target \n",
    "        :params enc_voc_size: number of vocabs that encoderEmbedder can handle\n",
    "        :params dec_voc_size: number of vocabs that decoderEmbedder can handle\n",
    "        :params ffn_hidden: hidden vector dimension for fastfeedforward layer \n",
    "        :params n_layers: number of EncoderLayer/DecoderLayer used\n",
    "        :params drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.src_pad_token = src_pad_token\n",
    "        self.trg_pad_token = trg_pad_token\n",
    "        self.trg_sos_token = trg_sos_token\n",
    "        self.device = device\n",
    "        self.n_head = n_head\n",
    "        self.encoder = Encoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, enc_voc_size=enc_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "        self.decoder = Decoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, dec_voc_size=dec_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "\n",
    "    def make_src_mask(self, src): \n",
    "        # print(f'src: {src}')\n",
    "        # print(f'src_pad_token: {self.src_pad_token}')\n",
    "        # print(f'src != self.src_pad_token: {src != self.src_pad_token}')\n",
    "        src_mask = (src != self.src_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg): \n",
    "        trg_pad_mask = (trg != self.trg_pad_token).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        # make a look-ahead mask using torch.tril \n",
    "        # [[1 0 0]\n",
    "        #  [1 1 0]\n",
    "        #  [1 1 1]]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "        \n",
    "    \n",
    "    def forward(self, src, trg): \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904213a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5469e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf203100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.178258Z",
     "iopub.status.busy": "2024-11-18T06:41:34.177976Z",
     "iopub.status.idle": "2024-11-18T06:41:34.190258Z",
     "shell.execute_reply": "2024-11-18T06:41:34.189393Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.178228Z"
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    "from collections import Counter \n",
    "import numpy \n",
    "\n",
    "# compute the statistics for BLEU \n",
    "def bleu_stats(hypothesis, reference): \n",
    "    stats = [] \n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "\n",
    "    for n in range(1, 5): \n",
    "        s_ngrams = Counter(\n",
    "            [tuple(hypothesis[i:i+n]) for i in range(len(hypothesis) + 1 - n)]\n",
    "        )\n",
    "\n",
    "        r_ngrams = Counter(\n",
    "            [tuple(reference[i:i+n]) for i in range(len(reference) + 1 - n)]\n",
    "        )\n",
    "\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
    "\n",
    "    return stats \n",
    "\n",
    "def bleu(stats): \n",
    "    for i in stats:\n",
    "        if i == 0:\n",
    "            return 0 \n",
    "\n",
    "    (h_len, r_len) = stats[:2]\n",
    "    log_bleu_prec = sum(\n",
    "        [math.log(float(x)/y) for x, y in zip(stats[2::2], stats[3::2])]\n",
    "    ) / 4.\n",
    "\n",
    "    return math.exp(min([0, 1 - float(r_len) / h_len]) + log_bleu_prec)\n",
    "\n",
    "def get_bleu(hypotheses, reference):\n",
    "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return 100 * bleu(stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e9562",
   "metadata": {},
   "source": [
    "# Train the Model using datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97726263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.191724Z",
     "iopub.status.busy": "2024-11-18T06:41:34.191396Z",
     "iopub.status.idle": "2024-11-18T06:41:34.201324Z",
     "shell.execute_reply": "2024-11-18T06:41:34.200532Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.191685Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "import torch \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "906d93bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.202720Z",
     "iopub.status.busy": "2024-11-18T06:41:34.202391Z",
     "iopub.status.idle": "2024-11-18T06:41:34.210411Z",
     "shell.execute_reply": "2024-11-18T06:41:34.209521Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.202672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA as device\n"
     ]
    }
   ],
   "source": [
    "# define device \n",
    "# configuration \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "144ae6e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.212080Z",
     "iopub.status.busy": "2024-11-18T06:41:34.211762Z",
     "iopub.status.idle": "2024-11-18T06:41:34.221256Z",
     "shell.execute_reply": "2024-11-18T06:41:34.220495Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.212049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define some configuration of training \n",
    "d_model = 256 \n",
    "n_head = 8\n",
    "max_len = 128\n",
    "ffn_hidden = 256 \n",
    "n_layers=6\n",
    "drop_prob=0.1\n",
    "epochs=300\n",
    "init_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "clip = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913997ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.223162Z",
     "iopub.status.busy": "2024-11-18T06:41:34.222341Z",
     "iopub.status.idle": "2024-11-18T06:41:34.229870Z",
     "shell.execute_reply": "2024-11-18T06:41:34.229087Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.223114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_pad_token: 0\n",
      "trg_pad_token: 0\n",
      "trg_sos_token: 102\n",
      "enc_voc_size: 42000\n",
      "dec_voc_size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Define some configuration of training \n",
    "\n",
    "src_pad_token = kr_tokenizer.pad_token_id\n",
    "trg_pad_token = en_tokenizer.pad_token_id\n",
    "trg_sos_token = en_tokenizer.sep_token_id\n",
    "enc_voc_size = kr_tokenizer.vocab_size\n",
    "dec_voc_size = en_tokenizer.vocab_size\n",
    "\n",
    "print(f'src_pad_token: {src_pad_token}')\n",
    "print(f'trg_pad_token: {trg_pad_token}')\n",
    "print(f'trg_sos_token: {trg_sos_token}')\n",
    "print(f'enc_voc_size: {enc_voc_size}')\n",
    "print(f'dec_voc_size: {dec_voc_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08da148a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.231154Z",
     "iopub.status.busy": "2024-11-18T06:41:34.230887Z",
     "iopub.status.idle": "2024-11-18T06:41:34.621978Z",
     "shell.execute_reply": "2024-11-18T06:41:34.621037Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.231125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter #: 32741178\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model \n",
    "model = Transformer(src_pad_token, trg_pad_token, trg_sos_token, enc_voc_size, dec_voc_size,n_head, max_len, ffn_hidden, n_layers, drop_prob, device).to(device)\n",
    "model.train()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'model parameter #: {count_parameters(model)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3809928c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:34.623459Z",
     "iopub.status.busy": "2024-11-18T06:41:34.623150Z",
     "iopub.status.idle": "2024-11-18T06:41:35.464954Z",
     "shell.execute_reply": "2024-11-18T06:41:35.463897Z",
     "shell.execute_reply.started": "2024-11-18T06:41:34.623428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup optimizer \n",
    "optimizer = Adam(params=model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=src_pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3173871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:35.466708Z",
     "iopub.status.busy": "2024-11-18T06:41:35.466163Z",
     "iopub.status.idle": "2024-11-18T06:41:35.479646Z",
     "shell.execute_reply": "2024-11-18T06:41:35.478541Z",
     "shell.execute_reply.started": "2024-11-18T06:41:35.466673Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch_num): \n",
    "    train_epoch_loss = 0 \n",
    "\n",
    "    for step, (kr_tokenized, en_tokenized) in tqdm(enumerate(train_dataloader)): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kr_tokenized = kr_tokenized.to(device)\n",
    "        en_tokenized = en_tokenized.to(device)\n",
    "\n",
    "        out = model(kr_tokenized, en_tokenized[:, :-1])\n",
    "\n",
    "        # remove sos token from en_tokenized when calculating loss because out will not include sos token. \n",
    "        en_tokenized = en_tokenized[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        out = out.contiguous().view(-1, out.shape[-1])\n",
    "\n",
    "        loss = loss_func(out.to(device), en_tokenized.type(torch.LongTensor).to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'EPOCH #{epoch_num} STEP #{step} | loss: {loss.item()}, avg_loss: {train_epoch_loss / (step + 1)}')\n",
    "\n",
    "    train_step_loss = train_epoch_loss / (step+1)\n",
    "    # After training epoch, do evaluation \n",
    "\n",
    "    return train_step_loss\n",
    "    \n",
    "\n",
    "# evaluate the model \n",
    "def evaluate(): \n",
    "    model.eval()\n",
    "    test_epoch_loss = 0 \n",
    "    test_bleu_loss = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for step, (kr_tokenized, en_tokenized) in tqdm(enumerate(test_dataloader)): \n",
    "            kr_tokenized = kr_tokenized.to(device)\n",
    "            en_tokenized = en_tokenized.to(device)\n",
    "\n",
    "            out = model(kr_tokenized, en_tokenized[:, :-1])\n",
    "\n",
    "            # remove sos token from en_tokenized when calculating loss because out will not include sos token. \n",
    "            en_tokenized = en_tokenized[:, 1:].contiguous().view(-1)\n",
    "    \n",
    "            out = out.contiguous().view(-1, out.shape[-1])\n",
    "            loss = loss_func(out.to(device), en_tokenized.type(torch.LongTensor).to(device))\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "            # calcuate the bleu \n",
    "            # TODO\n",
    "    return test_step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f477a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T06:41:35.481399Z",
     "iopub.status.busy": "2024-11-18T06:41:35.481015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #0 STEP #0 | loss: 10.50394058227539, avg_loss: 10.50394058227539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:33,  2.15it/s]"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(epoch)\n",
    "    test_loss = evaluate()\n",
    "\n",
    "    best_vloss = 100_000_000\n",
    "\n",
    "    print(f'Epoch {epoch}: Train Loss {train_loss}, Test Loss {test_loss}')\n",
    "\n",
    "    if test_loss < best_vloss:\n",
    "        best_vloss = avg_vloss \n",
    "        model_path = f'/kaggle/working/model_{timestamp}_{epoch}' \n",
    "        torch.save(model.state_dict(), model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa891694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6107863,
     "sourceId": 9938259,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
