{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9938705,"sourceType":"datasetVersion","datasetId":6107863}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizerFast, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd \nimport os \nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:13.732493Z","iopub.execute_input":"2024-11-18T07:22:13.732859Z","iopub.status.idle":"2024-11-18T07:22:16.496947Z","shell.execute_reply.started":"2024-11-18T07:22:13.732824Z","shell.execute_reply":"2024-11-18T07:22:16.495799Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:16.502125Z","iopub.execute_input":"2024-11-18T07:22:16.502434Z","iopub.status.idle":"2024-11-18T07:22:16.507014Z","shell.execute_reply.started":"2024-11-18T07:22:16.502400Z","shell.execute_reply":"2024-11-18T07:22:16.506075Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# define device \n# configuration \n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA as device\")\nelse:\n    # Check that MPS is available\n    if not torch.backends.mps.is_available():\n        if not torch.backends.mps.is_built():\n            print(\"MPS not available because the current PyTorch install was not \"\n                  \"built with MPS enabled.\")\n        else:\n            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n                  \"and/or you do not have an MPS-enabled device on this machine.\")\n        device = torch.device(\"cpu\")\n        print(\"Using CPU as device\")\n    else:\n        device = torch.device(\"mps\")\n        print(\"Using MPS as device\")\n    \ntorch.set_default_device(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:16.508064Z","iopub.execute_input":"2024-11-18T07:22:16.508364Z","iopub.status.idle":"2024-11-18T07:22:16.552092Z","shell.execute_reply.started":"2024-11-18T07:22:16.508318Z","shell.execute_reply":"2024-11-18T07:22:16.551186Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using CUDA as device\n","output_type":"stream"}]},{"cell_type":"code","source":"current_path = os.getcwd()\npreprocessed_directory = preprocessed_directory = os.path.join(current_path, \"preprocessed\")","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:16.554531Z","iopub.execute_input":"2024-11-18T07:22:16.554873Z","iopub.status.idle":"2024-11-18T07:22:16.561333Z","shell.execute_reply.started":"2024-11-18T07:22:16.554841Z","shell.execute_reply":"2024-11-18T07:22:16.560384Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n# import tokenizers\nkr_tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\nen_tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:16.562419Z","iopub.execute_input":"2024-11-18T07:22:16.562745Z","iopub.status.idle":"2024-11-18T07:22:16.940119Z","shell.execute_reply.started":"2024-11-18T07:22:16.562713Z","shell.execute_reply":"2024-11-18T07:22:16.939083Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test tokenizers \ntmp_kr_sentence = \"오늘 하교길에 길고양이를 보았는데, 너무 귀여워서 집에 데려가고 싶었다. 하지만 그러지는 않았다.\"\ntmp_en_sentence = \"The cat I saw during heading home today was so cute, that I wanted to bring it to home.\"\n\ntmp_kr_tokenized = kr_tokenizer(tmp_kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\ntmp_en_tokenized = en_tokenizer(tmp_en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n\n# print(kr_tokenizer.convert_ids_to_tokens(tmp_kr_tokenized.input_ids))\n# print(en_tokenizer.convert_ids_to_tokens(tmp_en_tokenized.input_ids))\n\n# print(kr_tokenizer.decode(tmp_kr_tokenized.input_ids, skip_special_tokens=True))\n\n# check if both tokenizer has pad token \n# print(kr_tokenizer.pad_token)\n# print(en_tokenizer.pad_token)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:16.941299Z","iopub.execute_input":"2024-11-18T07:22:16.941641Z","iopub.status.idle":"2024-11-18T07:22:16.948489Z","shell.execute_reply.started":"2024-11-18T07:22:16.941579Z","shell.execute_reply":"2024-11-18T07:22:16.947583Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_parquet(path=\"/kaggle/input/train.parquet\")\ndf_test = pd.read_parquet(path=\"/kaggle/input/test.parquet\")\ndf_validation = pd.read_parquet(path=\"/kaggle/input/validation.parquet\")\n\nclass en2kr_Train_Dataset(Dataset): \n    def __init__(self, max_len): \n        self.data = df_train\n        self.max_len = max_len \n        self.kr_tokenizer = kr_tokenizer\n        self.en_tokenizer = en_tokenizer\n        \n    def __len__(self): \n        return len(self.data) \n\n    def __getitem__(self, idx): \n        row = self.data.iloc[[idx]]\n        en_sentence = row[\"english\"].item()\n        kr_sentence = row[\"korean\"].item()\n        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n\n        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n\n        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n        return kr_tokenized_ids, en_tokenized_ids\n        \nclass en2kr_Test_Dataset(Dataset): \n    def __init__(self, max_len): \n        self.data = df_test\n        self.max_len = max_len \n        self.kr_tokenizer = kr_tokenizer\n        self.en_tokenizer = en_tokenizer\n        \n    def __len__(self): \n        return len(self.data) \n\n    def __getitem__(self, idx): \n        row = self.data.iloc[[idx]]\n        en_sentence = row[\"english\"].item()\n        kr_sentence = row[\"korean\"].item()\n        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n\n        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n        \n        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n        \n        return kr_tokenized_ids, en_tokenized_ids\n\nclass en2kr_Validation_Dataset(Dataset): \n    def __init__(self, max_len): \n        self.data = df_validation\n        self.max_len = max_len \n        self.kr_tokenizer = kr_tokenizer\n        self.en_tokenizer = en_tokenizer\n        \n    def __len__(self): \n        return len(self.data) \n\n    def __getitem__(self, idx): \n        row = self.data.iloc[[idx]]\n        en_sentence = row[\"english\"].item()\n        kr_sentence = row[\"korean\"].item()\n        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n\n        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n        \n        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n        \n        return kr_tokenized_ids, en_tokenized_ids","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:16.950083Z","iopub.execute_input":"2024-11-18T07:22:16.950536Z","iopub.status.idle":"2024-11-18T07:22:20.880880Z","shell.execute_reply.started":"2024-11-18T07:22:16.950494Z","shell.execute_reply":"2024-11-18T07:22:20.879767Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\ntrain_dataset = en2kr_Train_Dataset(max_len=128)\ntest_dataset = en2kr_Test_Dataset(max_len=128)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True,  shuffle=True, generator=torch.Generator(device=device), pin_memory=True, num_workers=4)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, generator=torch.Generator(device=device), pin_memory=True, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:20.882303Z","iopub.execute_input":"2024-11-18T07:22:20.882650Z","iopub.status.idle":"2024-11-18T07:22:20.888926Z","shell.execute_reply.started":"2024-11-18T07:22:20.882590Z","shell.execute_reply":"2024-11-18T07:22:20.887999Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Model Implementation \n","metadata":{}},{"cell_type":"code","source":"# import required packages\nimport torch \nimport torch.nn as nn \nimport copy \nimport math \nfrom torch.nn.functional import log_softmax\nimport pandas as pd \nfrom torch.utils.data import Dataset, DataLoader\nimport spacy \n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:20.890133Z","iopub.execute_input":"2024-11-18T07:22:20.890426Z","iopub.status.idle":"2024-11-18T07:22:22.493641Z","shell.execute_reply.started":"2024-11-18T07:22:20.890388Z","shell.execute_reply":"2024-11-18T07:22:22.492804Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define a Token Embedding \nclass TokenEmbeddings(nn.Embedding): \n    \"\"\"\n    Converting token into embedding vector\n    \"\"\"\n    def __init__(self, vocab_size, d_model):\n        \"\"\"\n        class for token embedding without positional encoding\n\n        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n        :param d_model: dimension of embedding vector\n        \"\"\"\n        super(TokenEmbeddings, self).__init__(vocab_size, d_model, padding_idx=1)\n\n# Define Positional Encoding \nclass PositionalEncoding(nn.Module): \n    \"\"\" \n    compute reusable sinusoid positional encoding\n    \"\"\"\n    def __init__(self, d_model, max_len, device): \n        \"\"\"\n        construct sinusoid positional encoding that is going to be reused everytime when it is needed\n\n        :param d_model: dimension of embedding vector\n        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n\n        # define a max_len * d_model size encoding matrix\n        self.encoding = torch.zeros(max_len, d_model, device=device)\n\n        # since positional encoding is not learnable, we turn off the gradient engine\n        self.encoding.requires_grad = False\n\n        # define a position at the sequence\n        pos = torch.arange(0, max_len, device=device)\n        # expand the max_len vector to max_len * 1 matrix \n        pos = pos.float().unsqueeze(dim=1)\n\n        _2i = torch.arange(0, d_model, step=2, device=device).float()\n\n        # define a sinusoid positional encoding\n        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n\n    def forward(self, x): \n        batch_size, seq_len = x.shape\n\n        return self.encoding[:seq_len, :]\n\n# Define Transformer Embedding \nclass TransformerEmbedding(nn.Module): \n    \"\"\"\n    token embedding + positional encoding\n    \"\"\"\n    def __init__(self, vocab_size, d_model, max_len, drop_prob, device): \n        \"\"\"\n        initialize the embedding class for word+position embedding\n\n        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n        :param d_model: dimension of embedding vector\n        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n        :param drop_prob: dropout probability to reduce overfitting\n        \"\"\"\n        super(TransformerEmbedding, self).__init__()\n        self.token_emb = TokenEmbeddings(vocab_size, d_model)\n        self.position_emb = PositionalEncoding(d_model, max_len, device)\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x): \n        tok_emb = self.token_emb(x)\n        pos_emb = self.position_emb(x)\n\n        return self.dropout(tok_emb+pos_emb)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.494761Z","iopub.execute_input":"2024-11-18T07:22:22.495260Z","iopub.status.idle":"2024-11-18T07:22:22.655567Z","shell.execute_reply.started":"2024-11-18T07:22:22.495225Z","shell.execute_reply":"2024-11-18T07:22:22.654511Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define Attention Block \nclass AttentionBlock(nn.Module): \n    \"\"\"\n    compute scale dot product attention for Query, Key, Value\n    \"\"\"\n    def __init__(self):\n        super(AttentionBlock, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, q, k, v, mask=None, eps=1e-12): \n        batch_size, head, length, d_tensor = k.shape\n\n        # calculate the k_T\n        k_T = k.transpose(2, 3)\n\n        # calculate the attention weight \n        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n\n        # if there are any masks that needs to be applied\n        if mask is not None:\n            att_weight = att_weight.masked_fill(mask == 0, -10000)\n\n        # calculate the softmax \n        # att_weight shape: batch_size, head, seq_len, seq_len\n        att_weight = self.softmax(att_weight)\n\n        # att_weight @ v shape: batch_size, head, seq_len, d_model\n        return att_weight @ v, att_weight\n\n# Define MultiHeadAttention Block \nclass MultiHeadAttentionBlock(nn.Module): \n    \"\"\"\n    define multi head attention block using AttentionBlock module\n    \"\"\"\n    def __init__(self, d_model, n_head): \n        \"\"\"\n        Multi-head self-attention utilize the parallelism of GPU\n\n        :param d_model: dimension of embedding vector\n        :param n_head: number of heads\n        \"\"\"\n        super(MultiHeadAttentionBlock, self).__init__()\n        self.n_head = n_head\n        self.attention = AttentionBlock()\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n\n        # in the paper, d_v = d_k = d_q\n        self.Wv = nn.Linear(d_model, d_model)\n\n        self.Wconcat = nn.Linear(d_model, d_model)\n\n    def split(self, tensor): \n        \"\"\"\n        split the tensor by number of head \n\n        :param tensor: tensor of shape batch_size  * seq_len * d_model\n        :return: return tensor of shape batch_size * n_head * seq_len * d_tensor\n        \"\"\"\n        batch_size, seq_len, d_model = tensor.shape \n\n        d_tensor = d_model // self.n_head\n\n        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n\n        return tensor \n\n    def concat(self, tensor): \n        \"\"\"\n        concat tensor. Inverse operation of split\n\n        :param tensor: tensor of shape batch_size * n_head * seq_len * d_tensor \n        :return: return tensor of shape batch_size * seq_len * d_model\n        \"\"\"\n        batch_size, n_head, seq_len, d_tensor = tensor.shape\n\n        d_model = n_head * d_tensor\n        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n        return tensor \n    \n\n    def forward(self, q, k, v, mask=None): \n        # apply linear transformation to derive q, k, v \n        q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)\n\n        # split the tensor by number of heads\n        q, k, v = self.split(q), self.split(k), self.split(v)\n\n        # apply attention to q, k, v \n        out, att_weight = self.attention(q, k, v, mask=mask)\n\n        # concat \n        out = self.concat(out)\n\n        # apply concat weight \n        out = self.Wconcat(out)\n        return out \n        ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.656913Z","iopub.execute_input":"2024-11-18T07:22:22.657289Z","iopub.status.idle":"2024-11-18T07:22:22.672873Z","shell.execute_reply.started":"2024-11-18T07:22:22.657257Z","shell.execute_reply":"2024-11-18T07:22:22.671986Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define LayerNorm \nclass LayerNorm(nn.Module): \n    \"\"\"\n    Normalize all features for each samples. \n    \"\"\"\n    def __init__(self, d_model, eps=1e-12): \n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps \n\n    def forward(self, x): \n        mean = x.mean(-1, keepdim=True)\n        var = x.var(-1, keepdim=True, unbiased=False)\n\n        out = (x - mean) / torch.sqrt(var + self.eps)\n        out = self.gamma * out + self.beta\n\n        return out ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.673979Z","iopub.execute_input":"2024-11-18T07:22:22.674267Z","iopub.status.idle":"2024-11-18T07:22:22.685631Z","shell.execute_reply.started":"2024-11-18T07:22:22.674237Z","shell.execute_reply":"2024-11-18T07:22:22.684723Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# define FeedForward Network \nclass FeedForwardBlock(nn.Module): \n    def __init__(self, d_model, hidden, drop_prob=0.1): \n        super(FeedForwardBlock, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden) \n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU() \n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x): \n        x = self.linear1(x) \n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x) \n        return x ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.690069Z","iopub.execute_input":"2024-11-18T07:22:22.690349Z","iopub.status.idle":"2024-11-18T07:22:22.697004Z","shell.execute_reply.started":"2024-11-18T07:22:22.690318Z","shell.execute_reply":"2024-11-18T07:22:22.696144Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Define Encoder Layer \nclass EncoderLayer(nn.Module): \n    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttentionBlock(d_model, n_head)\n        self.norm1 = LayerNorm(d_model) \n        self.dropout1 = nn.Dropout(drop_prob)\n\n        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n        self.norm2 = LayerNorm(d_model)\n        self.dropout2 = nn.Dropout(drop_prob)\n\n    def forward(self, x, src_mask): \n        residual = x \n        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n\n        x = self.dropout1(x) \n        x = self.norm1(x + residual)\n\n        residual = x \n        x = self.ffn(x) \n\n        x =  self.dropout2(x)\n        x = self.norm2(x + residual)\n\n        return x \n\n# Define Decoder Layer \nclass DecoderLayer(nn.Module): \n    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttentionBlock(d_model, n_head)\n        self.norm1 = LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.enc_dec_attention = MultiHeadAttentionBlock(d_model, n_head)\n        self.norm2 = LayerNorm(d_model)\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n        self.ffn = FeedForwardBlock(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm3 = LayerNorm(d_model)\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, dec, enc, trg_mask, src_mask): \n        residual = dec\n        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n\n        x = self.dropout1(x)\n        x = self.norm1(x + residual)\n\n        if enc is not None: \n            residual = x \n            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n            x = self.dropout2(x) \n            x = self.norm2(x + residual)\n\n        residual = x \n        x = self.ffn(x)\n        x = self.dropout3(x)\n        x = self.norm3(x + residual)\n\n        return x ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.698359Z","iopub.execute_input":"2024-11-18T07:22:22.698860Z","iopub.status.idle":"2024-11-18T07:22:22.713270Z","shell.execute_reply.started":"2024-11-18T07:22:22.698817Z","shell.execute_reply":"2024-11-18T07:22:22.712408Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define Encoder Model\nclass Encoder(nn.Module): \n    \"\"\"\n    Encoder for Transformer\n    \"\"\"\n    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n        super(Encoder, self).__init__()\n        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=enc_voc_size, drop_prob=drop_prob, device=device)\n        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n\n    def forward(self, x, src_mask): \n        x = self.emb(x) \n        for layer in self.layers: \n            x = layer(x, src_mask)\n\n        return x\n\n        \nclass Decoder(nn.Module): \n    \"\"\"\n    Decoder for Transformer\n    \"\"\"\n    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n        super(Decoder, self).__init__()\n        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=dec_voc_size, drop_prob=drop_prob, device=device)\n\n        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n\n        self.linear = nn.Linear(d_model, dec_voc_size)\n\n    def forward(self, trg, enc_src, trg_mask, src_mask): \n        trg = self.emb(trg)\n        for layer in self.layers: \n            trg = layer(trg, enc_src, trg_mask, src_mask) \n\n        output = self.linear(trg)\n        return output\n    \n\n# Define Transformer Model \nclass Transformer(nn.Module): \n    \"\"\"\n    Transformer Model\n    \"\"\"\n    def __init__(self, src_pad_token, trg_pad_token, trg_sos_token, enc_voc_size, dec_voc_size, n_head, max_len, d_model, ffn_hidden, n_layers, drop_prob, device): \n        \"\"\"\n        Constructing Transformer Model \n\n        :param src_pad_token: embedding vector that represents <pad> in source \n        :param trg_pad_token: embedding vector that represents <pad> in target \n        :param trg_sos_token: embedding vector that represents <sos> in target \n        :params enc_voc_size: number of vocabs that encoderEmbedder can handle\n        :params dec_voc_size: number of vocabs that decoderEmbedder can handle\n        :params ffn_hidden: hidden vector dimension for fastfeedforward layer \n        :params n_layers: number of EncoderLayer/DecoderLayer used\n        :params drop_prob: dropout probability\n        \"\"\"\n        super(Transformer, self).__init__()\n\n        self.src_pad_token = src_pad_token\n        self.trg_pad_token = trg_pad_token\n        self.trg_sos_token = trg_sos_token\n        self.device = device\n        self.n_head = n_head\n        self.encoder = Encoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, enc_voc_size=enc_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n        self.decoder = Decoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, dec_voc_size=dec_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n\n    def make_src_mask(self, src): \n        # print(f'src: {src}')\n        # print(f'src_pad_token: {self.src_pad_token}')\n        # print(f'src != self.src_pad_token: {src != self.src_pad_token}')\n        src_mask = (src != self.src_pad_token).unsqueeze(1).unsqueeze(2)\n        return src_mask\n\n    def make_trg_mask(self, trg): \n        trg_pad_mask = (trg != self.trg_pad_token).unsqueeze(1).unsqueeze(3)\n        trg_len = trg.shape[1]\n\n        # make a look-ahead mask using torch.tril \n        # [[1 0 0]\n        #  [1 1 0]\n        #  [1 1 1]]\n        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n\n        trg_mask = trg_pad_mask & trg_sub_mask\n        return trg_mask\n        \n    \n    def forward(self, src, trg): \n        src_mask = self.make_src_mask(src)\n        trg_mask = self.make_trg_mask(trg)\n        enc_src = self.encoder(src, src_mask)\n        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.714705Z","iopub.execute_input":"2024-11-18T07:22:22.714991Z","iopub.status.idle":"2024-11-18T07:22:22.733839Z","shell.execute_reply.started":"2024-11-18T07:22:22.714961Z","shell.execute_reply":"2024-11-18T07:22:22.732876Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math \nfrom collections import Counter \nimport numpy \n\n# compute the statistics for BLEU \ndef bleu_stats(hypothesis, reference): \n    stats = [] \n    stats.append(len(hypothesis))\n    stats.append(len(reference))\n\n    for n in range(1, 5): \n        s_ngrams = Counter(\n            [tuple(hypothesis[i:i+n]) for i in range(len(hypothesis) + 1 - n)]\n        )\n\n        r_ngrams = Counter(\n            [tuple(reference[i:i+n]) for i in range(len(reference) + 1 - n)]\n        )\n\n        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n        stats.append(max([len(hypothesis) + 1 - n, 0]))\n\n    return stats \n\ndef bleu(stats): \n    for i in stats:\n        if i == 0:\n            return 0 \n\n    (h_len, r_len) = stats[:2]\n    log_bleu_prec = sum(\n        [math.log(float(x)/y) for x, y in zip(stats[2::2], stats[3::2])]\n    ) / 4.\n\n    return math.exp(min([0, 1 - float(r_len) / h_len]) + log_bleu_prec)\n\ndef get_bleu(hypotheses, reference):\n    \"\"\"Get validation BLEU score for dev set.\"\"\"\n    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n    for hyp, ref in zip(hypotheses, reference):\n        stats += np.array(bleu_stats(hyp, ref))\n    return 100 * bleu(stats)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.734884Z","iopub.execute_input":"2024-11-18T07:22:22.735209Z","iopub.status.idle":"2024-11-18T07:22:22.750484Z","shell.execute_reply.started":"2024-11-18T07:22:22.735178Z","shell.execute_reply":"2024-11-18T07:22:22.749658Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model using datas","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\nfrom datetime import datetime\nimport torch \nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.751527Z","iopub.execute_input":"2024-11-18T07:22:22.751820Z","iopub.status.idle":"2024-11-18T07:22:22.763074Z","shell.execute_reply.started":"2024-11-18T07:22:22.751790Z","shell.execute_reply":"2024-11-18T07:22:22.762352Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# define device \n# configuration \n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA as device\")\nelse:\n    # Check that MPS is available\n    if not torch.backends.mps.is_available():\n        if not torch.backends.mps.is_built():\n            print(\"MPS not available because the current PyTorch install was not \"\n                  \"built with MPS enabled.\")\n        else:\n            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n                  \"and/or you do not have an MPS-enabled device on this machine.\")\n        device = torch.device(\"cpu\")\n        print(\"Using CPU as device\")\n    else:\n        device = torch.device(\"mps\")\n        print(\"Using MPS as device\")\n    \ntorch.set_default_device(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.764088Z","iopub.execute_input":"2024-11-18T07:22:22.764372Z","iopub.status.idle":"2024-11-18T07:22:22.775896Z","shell.execute_reply.started":"2024-11-18T07:22:22.764342Z","shell.execute_reply":"2024-11-18T07:22:22.775045Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Using CUDA as device\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define some configuration of training \nd_model = 256 \nn_head = 8\nmax_len = 128\nffn_hidden = 256 \nn_layers=6\ndrop_prob=0.1\nepochs=300\ninit_lr = 1e-3\nweight_decay = 5e-4\nclip = 1","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.776894Z","iopub.execute_input":"2024-11-18T07:22:22.777252Z","iopub.status.idle":"2024-11-18T07:22:22.790114Z","shell.execute_reply.started":"2024-11-18T07:22:22.777208Z","shell.execute_reply":"2024-11-18T07:22:22.789216Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Define some configuration of training \n\nsrc_pad_token = kr_tokenizer.pad_token_id\ntrg_pad_token = en_tokenizer.pad_token_id\ntrg_sos_token = en_tokenizer.sep_token_id\nenc_voc_size = kr_tokenizer.vocab_size\ndec_voc_size = en_tokenizer.vocab_size\n\nprint(f'src_pad_token: {src_pad_token}')\nprint(f'trg_pad_token: {trg_pad_token}')\nprint(f'trg_sos_token: {trg_sos_token}')\nprint(f'enc_voc_size: {enc_voc_size}')\nprint(f'dec_voc_size: {dec_voc_size}')","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.791297Z","iopub.execute_input":"2024-11-18T07:22:22.791580Z","iopub.status.idle":"2024-11-18T07:22:22.801157Z","shell.execute_reply.started":"2024-11-18T07:22:22.791550Z","shell.execute_reply":"2024-11-18T07:22:22.800228Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"src_pad_token: 0\ntrg_pad_token: 0\ntrg_sos_token: 102\nenc_voc_size: 42000\ndec_voc_size: 30522\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prepare the model \nmodel = Transformer(src_pad_token, trg_pad_token, trg_sos_token, enc_voc_size, dec_voc_size, n_head, max_len, d_model, ffn_hidden, n_layers, drop_prob, device).to(device)\nmodel.train()\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'model parameter #: {count_parameters(model)}')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:22.802293Z","iopub.execute_input":"2024-11-18T07:22:22.802993Z","iopub.status.idle":"2024-11-18T07:22:23.009123Z","shell.execute_reply.started":"2024-11-18T07:22:22.802944Z","shell.execute_reply":"2024-11-18T07:22:23.008177Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"model parameter #: 32741178\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setup optimizer \noptimizer = Adam(params=model.parameters(), lr=init_lr, weight_decay=weight_decay)\n\nloss_func = nn.CrossEntropyLoss(ignore_index=src_pad_token)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:23.010347Z","iopub.execute_input":"2024-11-18T07:22:23.010751Z","iopub.status.idle":"2024-11-18T07:22:23.653070Z","shell.execute_reply.started":"2024-11-18T07:22:23.010713Z","shell.execute_reply":"2024-11-18T07:22:23.652261Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def train_epoch(epoch_num): \n    train_epoch_loss = 0 \n\n    for step, (kr_tokenized, en_tokenized) in tqdm(enumerate(train_dataloader)): \n        optimizer.zero_grad()\n\n        kr_tokenized = kr_tokenized.to(device)\n        en_tokenized = en_tokenized.to(device)\n\n        out = model(kr_tokenized, en_tokenized[:, :-1])\n\n        # remove sos token from en_tokenized when calculating loss because out will not include sos token. \n        en_tokenized = en_tokenized[:, 1:].contiguous().view(-1)\n\n        out = out.contiguous().view(-1, out.shape[-1])\n\n        loss = loss_func(out.to(device), en_tokenized.type(torch.LongTensor).to(device))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        optimizer.step()\n\n        train_epoch_loss += loss.item()\n        \n        if step % 100 == 0:\n            print(f'EPOCH #{epoch_num} STEP #{step} | loss: {loss.item()}, avg_loss: {train_epoch_loss / (step + 1)}')\n\n    train_step_loss = train_epoch_loss / (step+1)\n    # After training epoch, do evaluation \n\n    return train_step_loss\n    \n\n# evaluate the model \ndef evaluate(): \n    model.eval()\n    test_epoch_loss = 0 \n    test_bleu_loss = 0\n    \n    with torch.no_grad(): \n        for step, (kr_tokenized, en_tokenized) in tqdm(enumerate(test_dataloader)): \n            kr_tokenized = kr_tokenized.to(device)\n            en_tokenized = en_tokenized.to(device)\n\n            out = model(kr_tokenized, en_tokenized[:, :-1])\n\n            # remove sos token from en_tokenized when calculating loss because out will not include sos token. \n            en_tokenized = en_tokenized[:, 1:].contiguous().view(-1)\n    \n            out = out.contiguous().view(-1, out.shape[-1])\n            loss = loss_func(out.to(device), en_tokenized.type(torch.LongTensor).to(device))\n            test_epoch_loss += loss.item()\n\n            # calcuate the bleu \n            # TODO\n    return test_step_loss","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:23.654269Z","iopub.execute_input":"2024-11-18T07:22:23.654815Z","iopub.status.idle":"2024-11-18T07:22:23.666421Z","shell.execute_reply.started":"2024-11-18T07:22:23.654777Z","shell.execute_reply":"2024-11-18T07:22:23.665494Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\nfor epoch in range(epochs):\n    train_loss = train_epoch(epoch)\n    test_loss = evaluate()\n\n    best_vloss = 100_000_000\n\n    print(f'Epoch {epoch}: Train Loss {train_loss}, Test Loss {test_loss}')\n\n    if test_loss < best_vloss:\n        best_vloss = avg_vloss \n        model_path = f'/kaggle/working/model_{timestamp}_{epoch}' \n        torch.save(model.state_dict(), model_path)  ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T07:22:23.667536Z","iopub.execute_input":"2024-11-18T07:22:23.667875Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"1it [00:01,  1.10s/it]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #0 | loss: 10.46674919128418, avg_loss: 10.46674919128418\n","output_type":"stream"},{"name":"stderr","text":"101it [00:47,  2.15it/s]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #100 | loss: 6.184701442718506, avg_loss: 6.895652855976974\n","output_type":"stream"},{"name":"stderr","text":"201it [01:34,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #200 | loss: 5.654942512512207, avg_loss: 6.381246419688362\n","output_type":"stream"},{"name":"stderr","text":"301it [02:20,  2.15it/s]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #300 | loss: 5.396417140960693, avg_loss: 6.094745647075564\n","output_type":"stream"},{"name":"stderr","text":"401it [03:07,  2.15it/s]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #400 | loss: 5.4601521492004395, avg_loss: 5.914357733548133\n","output_type":"stream"},{"name":"stderr","text":"501it [03:53,  2.15it/s]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #500 | loss: 5.0429558753967285, avg_loss: 5.7819255668959935\n","output_type":"stream"},{"name":"stderr","text":"601it [04:40,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #600 | loss: 5.208008289337158, avg_loss: 5.6821281445799965\n","output_type":"stream"},{"name":"stderr","text":"701it [05:27,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"EPOCH #0 STEP #700 | loss: 5.175655841827393, avg_loss: 5.60462308304116\n","output_type":"stream"},{"name":"stderr","text":"784it [06:05,  2.13it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}