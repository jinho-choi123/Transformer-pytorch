{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c95f00f-c1f3-422b-a701-56b8c8329eac",
   "metadata": {},
   "source": [
    "# Transformer Model Implementation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05dfc35a-d661-4bc4-a549-d1476c22fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import copy \n",
    "import math \n",
    "from torch.nn.functional import log_softmax\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b9255b-2fc1-4eb9-9b97-ab9a2723ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Token Embedding \n",
    "class TokenEmbeddings(nn.Embedding): \n",
    "    \"\"\"\n",
    "    Converting token into embedding vector\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        class for token embedding without positional encoding\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super(TokenEmbeddings, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "\n",
    "# Define Positional Encoding \n",
    "class PositionalEncoding(nn.Module): \n",
    "    \"\"\" \n",
    "    compute reusable sinusoid positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len, device): \n",
    "        \"\"\"\n",
    "        construct sinusoid positional encoding that is going to be reused everytime when it is needed\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # define a max_len * d_model size encoding matrix\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "\n",
    "        # since positional encoding is not learnable, we turn off the gradient engine\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        # define a position at the sequence\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        # expand the max_len vector to max_len * 1 matrix \n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        # define a sinusoid positional encoding\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "# Define Transformer Embedding \n",
    "class TransformerEmbedding(nn.Module): \n",
    "    \"\"\"\n",
    "    token embedding + positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device): \n",
    "        \"\"\"\n",
    "        initialize the embedding class for word+position embedding\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        :param drop_prob: dropout probability to reduce overfitting\n",
    "        \"\"\"\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_emb = TokenEmbeddings(vocab_size, d_model)\n",
    "        self.position_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        tok_emb = self.token_emb(x)\n",
    "        pos_emb = self.position_emb(x)\n",
    "\n",
    "        return self.dropout(tok_emb+pos_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082e2d7c-b5d7-463e-a65e-dbcea5e75acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Attention Block \n",
    "class AttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    compute scale dot product attention for Query, Key, Value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, eps=1e-12): \n",
    "        batch_size, head, length, d_tensor = k.shape\n",
    "\n",
    "        # calculate the k_T\n",
    "        k_T = k.transpose(2, 3)\n",
    "\n",
    "        # calculate the attention weight \n",
    "        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n",
    "\n",
    "        # if there are any masks that needs to be applied\n",
    "        if mask is not None:\n",
    "            att_weight = att_weight.masked_fill(mask == 0, -10000)\n",
    "\n",
    "        # calculate the softmax \n",
    "        # att_weight shape: batch_size, head, seq_len, seq_len\n",
    "        att_weight = self.softmax(att_weight)\n",
    "\n",
    "        # att_weight @ v shape: batch_size, head, seq_len, d_model\n",
    "        return att_weight @ v, att_weight\n",
    "\n",
    "# Define MultiHeadAttention Block \n",
    "class MultiHeadAttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    define multi head attention block using AttentionBlock module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_head): \n",
    "        \"\"\"\n",
    "        Multi-head self-attention utilize the parallelism of GPU\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param n_head: number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = AttentionBlock()\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # in the paper, d_v = d_k = d_q\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.Wconcat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split(self, tensor): \n",
    "        \"\"\"\n",
    "        split the tensor by number of head \n",
    "\n",
    "        :param tensor: tensor of shape batch_size  * seq_len * d_model\n",
    "        :return: return tensor of shape batch_size * n_head * seq_len * d_tensor\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = tensor.shape \n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "\n",
    "        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor \n",
    "\n",
    "    def concat(self, tensor): \n",
    "        \"\"\"\n",
    "        concat tensor. Inverse operation of split\n",
    "\n",
    "        :param tensor: tensor of shape batch_size * n_head * seq_len * d_tensor \n",
    "        :return: return tensor of shape batch_size * seq_len * d_model\n",
    "        \"\"\"\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.shape\n",
    "\n",
    "        d_model = n_head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return tensor \n",
    "    \n",
    "\n",
    "    def forward(self, q, k, v, mask=None): \n",
    "        # apply linear transformation to derive q, k, v \n",
    "        q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)\n",
    "\n",
    "        # split the tensor by number of heads\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        # apply attention to q, k, v \n",
    "        out, att_weight = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # concat \n",
    "        out = self.concat(out)\n",
    "\n",
    "        # apply concat weight \n",
    "        out = self.Wconcat(out)\n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdf03e0-8bb9-4194-9a59-a7ba2546f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LayerNorm \n",
    "class LayerNorm(nn.Module): \n",
    "    \"\"\"\n",
    "    Normalize all features for each samples. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-12): \n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, x): \n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc7b94a-5680-4860-87df-a2665d85fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define FeedForward Network \n",
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1): \n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden) \n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1718d3-90ee-4c13-8363-76fd56515a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder Layer \n",
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm1 = LayerNorm(d_model) \n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask): \n",
    "        residual = x \n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "\n",
    "        x = self.dropout1(x) \n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x) \n",
    "\n",
    "        x =  self.dropout2(x)\n",
    "        x = self.norm2(x + residual)\n",
    "\n",
    "        return x \n",
    "\n",
    "# Define Decoder Layer \n",
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask): \n",
    "        residual = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        if enc is not None: \n",
    "            residual = x \n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "            x = self.dropout2(x) \n",
    "            x = self.norm2(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + residual)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca21171-0527-4604-ae4b-38d4174ff317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder Model\n",
    "class Encoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=enc_voc_size, drop_prob=drop_prob, device=device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask): \n",
    "        x = self.emb(x) \n",
    "        for layer in self.layers: \n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "        \n",
    "class Decoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=dec_voc_size, drop_prob=drop_prob, device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask): \n",
    "        trg = self.emb(trg)\n",
    "        for layer in self.layers: \n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask) \n",
    "\n",
    "        output = self.linear(trg)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Define Transformer Model \n",
    "class Transformer(nn.Module): \n",
    "    \"\"\"\n",
    "    Transformer Model\n",
    "    \"\"\"\n",
    "    def __init__(self, src_pad_token, trg_pad_token, trg_sos_token, enc_voc_size, dec_voc_size, ffn_hidden, n_layers, drop_prob, device): \n",
    "        \"\"\"\n",
    "        Constructing Transformer Model \n",
    "\n",
    "        :param src_pad_token: embedding vector that represents <pad> in source \n",
    "        :param trg_pad_token: embedding vector that represents <pad> in target \n",
    "        :param trg_sos_token: embedding vector that represents <sos> in target \n",
    "        :params enc_voc_size: number of vocabs that encoderEmbedder can handle\n",
    "        :params dec_voc_size: number of vocabs that decoderEmbedder can handle\n",
    "        :params ffn_hidden: hidden vector dimension for fastfeedforward layer \n",
    "        :params n_layers: number of EncoderLayer/DecoderLayer used\n",
    "        :params drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.src_pad_token = src_pad_token\n",
    "        self.trg_pad_token = trg_pad_token\n",
    "        self.trg_sos_token = trg_sos_token\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, enc_voc_size=enc_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "        self.decoder = Decoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, dec_voc_size=dec_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "\n",
    "    def make_src_mask(self, src): \n",
    "        # print(f'src: {src}')\n",
    "        # print(f'src_pad_token: {self.src_pad_token}')\n",
    "        # print(f'src != self.src_pad_token: {src != self.src_pad_token}')\n",
    "        src_mask = (src != self.src_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg): \n",
    "        trg_pad_mask = (trg != self.trg_pad_token).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        # make a look-ahead mask using torch.tril \n",
    "        # [[1 0 0]\n",
    "        #  [1 1 0]\n",
    "        #  [1 1 1]]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "        \n",
    "    \n",
    "    def forward(self, src, trg): \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81d49aa-5968-4b95-a9ed-f7f12967a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS as device\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fe859-957f-4306-bd5b-ede75622a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
