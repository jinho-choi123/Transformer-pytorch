{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab0035b-83a2-44f1-bb0a-100fc36d0be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ball/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd \n",
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88745aae-01d1-4fd9-8da3-cfba09d5e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS as device\n"
     ]
    }
   ],
   "source": [
    "# define device \n",
    "# configuration \n",
    "\n",
    "TOKENIZERS_PARALLELISM = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b657ad88-087b-4e22-b26d-b69b5a369c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "preprocessed_directory = preprocessed_directory = os.path.join(current_path, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b191bfd3-bedc-441e-a345-8eed33fbe22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import tokenizers\n",
    "kr_tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "en_tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff91b0d9-662c-45b5-bee2-f5ab77eca9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizers \n",
    "tmp_kr_sentence = \"오늘 하교길에 길고양이를 보았는데, 너무 귀여워서 집에 데려가고 싶었다. 하지만 그러지는 않았다.\"\n",
    "tmp_en_sentence = \"The cat I saw during heading home today was so cute, that I wanted to bring it to home.\"\n",
    "\n",
    "tmp_kr_tokenized = kr_tokenizer(tmp_kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "tmp_en_tokenized = en_tokenizer(tmp_en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "\n",
    "# print(kr_tokenizer.convert_ids_to_tokens(tmp_kr_tokenized.input_ids))\n",
    "# print(en_tokenizer.convert_ids_to_tokens(tmp_en_tokenized.input_ids))\n",
    "\n",
    "# print(kr_tokenizer.decode(tmp_kr_tokenized.input_ids, skip_special_tokens=True))\n",
    "\n",
    "# check if both tokenizer has pad token \n",
    "# print(kr_tokenizer.pad_token)\n",
    "# print(en_tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89ae1db-24b1-4c22-a532-a6bf8aa0d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(path=os.path.join(preprocessed_directory, \"train.parquet\"))\n",
    "df_test = pd.read_parquet(path=os.path.join(preprocessed_directory, \"test.parquet\"))\n",
    "df_validation = pd.read_parquet(path=os.path.join(preprocessed_directory, \"validation.parquet\"))\n",
    "\n",
    "\n",
    "\n",
    "class en2kr_Train_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_train\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "\n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        return kr_tokenized_ids, en_tokenized_ids\n",
    "        \n",
    "class en2kr_Test_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_test\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "        \n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        \n",
    "        return kr_tokenized_ids, en_tokenized_ids\n",
    "\n",
    "class en2kr_Validation_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_validation\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "        \n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        \n",
    "        return kr_tokenized_ids, en_tokenized_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb48b7b-bee2-4824-8984-45cd706d83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_dataset = en2kr_Train_Dataset(max_len=128)\n",
    "test_dataset = en2kr_Test_Dataset(max_len=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac0529-d529-46ce-a2dd-5f53c9e2859f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
