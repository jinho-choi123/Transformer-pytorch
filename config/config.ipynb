{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6876c6-7dc6-4e5c-87fb-2063e016b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10ac922e-ff7b-4168-96a8-557d3793e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@Configuration START@@\n"
     ]
    }
   ],
   "source": [
    "print(f'@@Configuration START@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0051fe2c-9f2f-4617-8e35-08206d015eb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 128\n",
      "d_model: 256\n",
      "n_head: 2\n",
      "max_len: 128\n",
      "ffn_hidden: 64\n",
      "n_layers: 6\n",
      "drop_prob: 0.1\n",
      "epochs: 300\n",
      "init_lr: 0.001\n",
      "weight_decay: 0.0005\n",
      "clip: 1\n"
     ]
    }
   ],
   "source": [
    "# Define configuration \n",
    "batch_size = 128\n",
    "d_model = 512 \n",
    "n_head = 8\n",
    "max_len = 128\n",
    "ffn_hidden = 1024 \n",
    "n_layers=6 \n",
    "drop_prob=0.1\n",
    "epochs=300\n",
    "init_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "clip = 1\n",
    "\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'd_model: {d_model}')\n",
    "print(f'n_head: {n_head}') \n",
    "print(f'max_len: {max_len}') \n",
    "print(f'ffn_hidden: {ffn_hidden}')\n",
    "print(f'n_layers: {n_layers}')\n",
    "print(f'drop_prob: {drop_prob}')\n",
    "print(f'epochs: {epochs}')\n",
    "print(f'init_lr: {init_lr}')\n",
    "print(f'weight_decay: {weight_decay}')\n",
    "print(f'clip: {clip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce37c2-0fc8-4d80-8c2d-bc5c6caf560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Device \n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559a9a2-6d1c-4fd7-ac94-5404c4ebb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizers\n",
    "TOKENIZERS_PARALLELISM = True\n",
    "\n",
    "kr_tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "en_tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "enc_voc_size = kr_tokenizer.vocab_size\n",
    "dec_voc_size = en_tokenizer.vocab_size\n",
    "\n",
    "print(f'Using kr_tokenizer: kykim/bert-kor-base')\n",
    "print(f'kr_tokenizer_voc_size(enc_voc_size): {enc_voc_size}')\n",
    "\n",
    "print(f'Using en_tokenizer: google-bert/bert-base-uncased')\n",
    "print(f'en_tokenizer_voc_size(dec_voc_size): {dec_voc_size}')\n",
    "\n",
    "# Define some variables that are going to be used in future\n",
    "src_pad_token = kr_tokenizer.pad_token_id\n",
    "trg_pad_token = en_tokenizer.pad_token_id\n",
    "trg_sos_token = en_tokenizer.sep_token_id\n",
    "\n",
    "print(f'src_pad_token: {src_pad_token}')\n",
    "print(f'trg_pad_token: {trg_pad_token}')\n",
    "print(f'trg_sos_token: {trg_sos_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1fa603-89e1-4824-9f2e-8e4d8ceb3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizers \n",
    "# tmp_kr_sentence = \"오늘 하교길에 길고양이를 보았는데, 너무 귀여워서 집에 데려가고 싶었다. 하지만 그러지는 않았다.\"\n",
    "# tmp_en_sentence = \"The cat I saw during heading home today was so cute, that I wanted to bring it to home.\"\n",
    "\n",
    "# tmp_kr_tokenized = kr_tokenizer(tmp_kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "# tmp_en_tokenized = en_tokenizer(tmp_en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "\n",
    "# print(kr_tokenizer.convert_ids_to_tokens(tmp_kr_tokenized.input_ids))\n",
    "# print(en_tokenizer.convert_ids_to_tokens(tmp_en_tokenized.input_ids))\n",
    "\n",
    "# print(kr_tokenizer.decode(tmp_kr_tokenized.input_ids, skip_special_tokens=True))\n",
    "\n",
    "# check if both tokenizer has pad token \n",
    "# print(kr_tokenizer.pad_token)\n",
    "# print(en_tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0691c4-13f2-4889-a6c1-bcb734016602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_dir: /Users/ball/Documents/workspace/transformer-tutorial\n",
      "rawdata_dir: /Users/ball/Documents/workspace/transformer-tutorial/data\n",
      "data_dir: /Users/ball/Documents/workspace/transformer-tutorial/preprocessed\n"
     ]
    }
   ],
   "source": [
    "# Define path configuration for the project \n",
    "project_dir = Path(os.getcwd()).parent\n",
    "rawdata_dir = project_dir / \"data\"\n",
    "data_dir = project_dir / \"preprocessed\"\n",
    "\n",
    "rawdata_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'project_dir: {project_dir}')\n",
    "print(f'rawdata_dir: {rawdata_dir}')\n",
    "print(f'data_dir: {data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4d284-8af6-4759-af29-afb86f7754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'@@Configuration END@@')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
