{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6876c6-7dc6-4e5c-87fb-2063e016b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ball/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import os\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ac922e-ff7b-4168-96a8-557d3793e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@Configuration START@@\n"
     ]
    }
   ],
   "source": [
    "print(f'@@Configuration START@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0051fe2c-9f2f-4617-8e35-08206d015eb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "d_model: 400\n",
      "n_head: 8\n",
      "max_len: 128\n",
      "ffn_hidden: 1200\n",
      "n_layers: 4\n",
      "drop_prob: 0.1\n",
      "epochs: 50\n",
      "init_lr: 0.0\n",
      "weight_decay: 0.0005\n",
      "clip: 1\n"
     ]
    }
   ],
   "source": [
    "# Define configuration \n",
    "batch_size = 64\n",
    "d_model = 256 \n",
    "n_head = 8\n",
    "max_len = 80\n",
    "ffn_hidden = 512\n",
    "n_layers=3 \n",
    "drop_prob=0.1\n",
    "epochs=30\n",
    "init_lr = 0.00 # having warmup step\n",
    "eps = 5e-9\n",
    "weight_decay = 5e-4\n",
    "warmup_steps=1500\n",
    "clip = 1\n",
    "\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'd_model: {d_model}')\n",
    "print(f'n_head: {n_head}') \n",
    "print(f'max_len: {max_len}') \n",
    "print(f'ffn_hidden: {ffn_hidden}')\n",
    "print(f'n_layers: {n_layers}')\n",
    "print(f'drop_prob: {drop_prob}')\n",
    "print(f'epochs: {epochs}')\n",
    "print(f'init_lr: {init_lr}')\n",
    "print(f'weight_decay: {weight_decay}')\n",
    "print(f'clip: {clip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbce37c2-0fc8-4d80-8c2d-bc5c6caf560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS as device\n"
     ]
    }
   ],
   "source": [
    "# Configure Device \n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b559a9a2-6d1c-4fd7-ac94-5404c4ebb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ball/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using kr_tokenizer: Helsinki-NLP/opus-mt-ko-en\n",
      "kr_tokenizer_voc_size(enc_voc_size): 65001\n",
      "Using en_tokenizer: Helsinki-NLP/opus-mt-ko-en\n",
      "en_tokenizer_voc_size(dec_voc_size): 65001\n",
      "src_pad_token: 65000\n",
      "src_eos_token: 0\n",
      "trg_pad_token: 65000\n",
      "trg_sos_token: None\n",
      "trg_eos_token: 0\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizers\n",
    "TOKENIZERS_PARALLELISM = True\n",
    "\n",
    "kr_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "en_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "\n",
    "enc_voc_size = kr_tokenizer.vocab_size\n",
    "dec_voc_size = en_tokenizer.vocab_size\n",
    "\n",
    "print(f'Using kr_tokenizer: Helsinki-NLP/opus-mt-ko-en')\n",
    "print(f'kr_tokenizer_voc_size(enc_voc_size): {enc_voc_size}')\n",
    "\n",
    "print(f'Using en_tokenizer: Helsinki-NLP/opus-mt-ko-en')\n",
    "print(f'en_tokenizer_voc_size(dec_voc_size): {dec_voc_size}')\n",
    "\n",
    "# Define some variables that are going to be used in future\n",
    "src_pad_token = kr_tokenizer.pad_token_id\n",
    "src_eos_token = kr_tokenizer.eos_token_id\n",
    "\n",
    "trg_pad_token = en_tokenizer.pad_token_id\n",
    "trg_sos_token = en_tokenizer.bos_token_id\n",
    "trg_eos_token = en_tokenizer.eos_token_id\n",
    "\n",
    "print(f'src_pad_token: {src_pad_token}')\n",
    "print(f'src_eos_token: {src_eos_token}')\n",
    "print(f'trg_pad_token: {trg_pad_token}')\n",
    "print(f'trg_sos_token: {trg_sos_token}')\n",
    "print(f'trg_eos_token: {trg_eos_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1fa603-89e1-4824-9f2e-8e4d8ceb3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizers \n",
    "tmp_kr_sentence = \"오늘 하교길에 길고양이를 보았는데, 너무 귀여워서 집에 데려가고 싶었다. 하지만 그러지는 않았다.\"\n",
    "tmp_en_sentence = \"The cat I saw during heading home today was so cute, that I wanted to bring it to home.\"\n",
    "\n",
    "tmp_kr_tokenized = kr_tokenizer(tmp_kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "# tmp_en_tokenized = en_tokenizer(tmp_en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "\n",
    "# print(tmp_kr_tokenized)\n",
    "# print(kr_tokenizer.convert_ids_to_tokens(tmp_kr_tokenized.input_ids))\n",
    "# print(en_tokenizer.convert_ids_to_tokens(tmp_en_tokenized.input_ids))\n",
    "# print(tmp_kr_tokenized.input_ids)\n",
    "# print(kr_tokenizer.decode(tmp_kr_tokenized.input_ids, skip_special_tokens=True))\n",
    "\n",
    "# check if both tokenizer has pad token \n",
    "# print(kr_tokenizer.pad_token)\n",
    "# print(en_tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0691c4-13f2-4889-a6c1-bcb734016602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_dir: /Users/ball/Documents/workspace/transformer-tutorial\n",
      "rawdata_dir: /Users/ball/Documents/workspace/transformer-tutorial/data\n",
      "data_dir: /Users/ball/Documents/workspace/transformer-tutorial/preprocessed\n"
     ]
    }
   ],
   "source": [
    "# Define path configuration for the project \n",
    "project_dir = Path(os.getcwd()).parent\n",
    "rawdata_dir = project_dir / \"data\"\n",
    "data_dir = project_dir / \"preprocessed\"\n",
    "model_dir = project_dir / \"models\"\n",
    "\n",
    "rawdata_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'project_dir: {project_dir}')\n",
    "print(f'rawdata_dir: {rawdata_dir}')\n",
    "print(f'data_dir: {data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f897540d-030a-4454-8118-8b6ea567c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate scheduler. \n",
    "# If you want to modify the logic of Scheduler, please modify this class \n",
    "\n",
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps, LR_scale=1): \n",
    "        self.optimizer = optimizer\n",
    "        self.step_count = 0 \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.LR_scale = LR_scale\n",
    "        self._d_model_factor = self.LR_scale * (self.d_model ** -0.5)\n",
    "    def step(self): \n",
    "        self.step_count += 1 \n",
    "        lr = self.calculate_learning_rate()\n",
    "        self.optimizer.param_groups[0]['lr'] = lr \n",
    "    def calculate_learning_rate(self): \n",
    "        minimum_factor = min(self.step_count ** -0.5, self.step_count * self.warmup_steps ** -1.5)\n",
    "        return self._d_model_factor * minimum_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c250b5cb-983b-4847-8567-3bc3632d2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Logger \n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "logging_dir = project_dir / \"logs\" \n",
    "\n",
    "logging_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = logging_dir / f'log_{timestamp}.log'\n",
    "\n",
    "logger = logging.getLogger('transformer_log')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d4d284-8af6-4759-af29-afb86f7754a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@Configuration END@@\n"
     ]
    }
   ],
   "source": [
    "print(f'@@Configuration END@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3cd074-f605-4727-8f3a-d4955ba4b095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ball/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@Configuration START@@\n",
      "batch_size: 64\n",
      "d_model: 400\n",
      "n_head: 8\n",
      "max_len: 128\n",
      "ffn_hidden: 1200\n",
      "n_layers: 4\n",
      "drop_prob: 0.1\n",
      "epochs: 50\n",
      "init_lr: 0.0\n",
      "weight_decay: 0.0005\n",
      "clip: 1\n",
      "Using MPS as device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ball/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using kr_tokenizer: Helsinki-NLP/opus-mt-ko-en\n",
      "kr_tokenizer_voc_size(enc_voc_size): 65001\n",
      "Using en_tokenizer: Helsinki-NLP/opus-mt-ko-en\n",
      "en_tokenizer_voc_size(dec_voc_size): 65001\n",
      "src_pad_token: 65000\n",
      "src_eos_token: 0\n",
      "trg_pad_token: 65000\n",
      "trg_sos_token: None\n",
      "trg_eos_token: 0\n",
      "project_dir: /Users/ball/Documents/workspace/transformer-tutorial\n",
      "rawdata_dir: /Users/ball/Documents/workspace/transformer-tutorial/data\n",
      "data_dir: /Users/ball/Documents/workspace/transformer-tutorial/preprocessed\n",
      "@@Configuration END@@\n"
     ]
    }
   ],
   "source": [
    "%run ../config/config.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab0035b-83a2-44f1-bb0a-100fc36d0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89ae1db-24b1-4c22-a532-a6bf8aa0d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = data_dir / \"train.parquet\"\n",
    "test_data_path = data_dir / \"test.parquet\"\n",
    "validation_data_path = data_dir / \"validation.parquet\"\n",
    "\n",
    "df_train = pd.read_parquet(path=train_data_path)\n",
    "df_test = pd.read_parquet(path=test_data_path)\n",
    "df_validation = pd.read_parquet(path=validation_data_path)\n",
    "\n",
    "class en2kr_Train_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_train\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        \n",
    "        return kr_sentence, en_sentence\n",
    "        \n",
    "class en2kr_Test_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_test\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        \n",
    "        return kr_sentence, en_sentence\n",
    "\n",
    "class en2kr_Validation_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_validation\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        \n",
    "        return kr_sentence, en_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f144d57d-2cbc-4528-bd8a-3daf4cec4717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>korean</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1239860</th>\n",
       "      <td>롯데면세점은 LDF페이 오픈기념으로 다채로운 증정혜택을 준비 중이다.</td>\n",
       "      <td>LOTTE Duty Free Shop is preparing a variety of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865809</th>\n",
       "      <td>너는 그저 위를 보고 스스로를 자랑스러워해.</td>\n",
       "      <td>Just look up and be proud of yourself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112285</th>\n",
       "      <td>왜 공개하지 못하겠다는 것인지 이세영 기자가 취재했습니다.</td>\n",
       "      <td>Reporter Lee Se-yeong covered why it insisted ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         korean  \\\n",
       "1239860  롯데면세점은 LDF페이 오픈기념으로 다채로운 증정혜택을 준비 중이다.   \n",
       "865809                 너는 그저 위를 보고 스스로를 자랑스러워해.   \n",
       "112285         왜 공개하지 못하겠다는 것인지 이세영 기자가 취재했습니다.   \n",
       "\n",
       "                                                   english  \n",
       "1239860  LOTTE Duty Free Shop is preparing a variety of...  \n",
       "865809              Just look up and be proud of yourself.  \n",
       "112285   Reporter Lee Se-yeong covered why it insisted ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb48b7b-bee2-4824-8984-45cd706d83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = en2kr_Train_Dataset(max_len=max_len)\n",
    "test_dataset = en2kr_Test_Dataset(max_len=max_len)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,pin_memory=True, drop_last=True, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "test_dataloader = DataLoader(test_dataset,pin_memory=True, drop_last=True, batch_size=batch_size, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95f00f-c1f3-422b-a701-56b8c8329eac",
   "metadata": {},
   "source": [
    "# Transformer Model Implementation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05dfc35a-d661-4bc4-a549-d1476c22fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70694b87-0e3e-4d64-b490-204c265a161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%run ../config/config.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b9255b-2fc1-4eb9-9b97-ab9a2723ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Token Embedding \n",
    "class TokenEmbeddings(nn.Embedding): \n",
    "    \"\"\"\n",
    "    Converting token into embedding vector\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        class for token embedding without positional encoding\n",
    "        This layer transforms an seq_len token_ids -> (seq_len, d_model)\n",
    "        Assigning (vector of size d_model) to each tokens\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super(TokenEmbeddings, self).__init__(vocab_size, d_model, padding_idx=65000)\n",
    "\n",
    "# Define Positional Encoding \n",
    "class PositionalEncoding(nn.Module): \n",
    "    \"\"\" \n",
    "    compute reusable sinusoid positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len, device): \n",
    "        \"\"\"\n",
    "        construct sinusoid positional encoding that is going to be reused everytime when it is needed\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # define a max_len * d_model size encoding matrix\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "\n",
    "        # since positional encoding is not learnable, we turn off the gradient engine\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        # define a position at the sequence\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        # expand the max_len vector to max_len * 1 matrix \n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        # define a sinusoid positional encoding\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "# Define Transformer Embedding \n",
    "class TransformerEmbedding(nn.Module): \n",
    "    \"\"\"\n",
    "    token embedding + positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device): \n",
    "        \"\"\"\n",
    "        initialize the embedding class for word+position embedding\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        :param drop_prob: dropout probability to reduce overfitting\n",
    "        \"\"\"\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_emb = TokenEmbeddings(vocab_size, d_model)\n",
    "        self.position_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x): \n",
    "        tok_emb = self.scale * self.token_emb(x)\n",
    "        pos_emb = self.position_emb(x)\n",
    "\n",
    "        return self.dropout(tok_emb+pos_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082e2d7c-b5d7-463e-a65e-dbcea5e75acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Attention Block \n",
    "class AttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    compute scale dot product attention for Query, Key, Value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, eps=1e-12): \n",
    "        batch_size, head, length, d_tensor = k.shape\n",
    "\n",
    "        # calculate the k_T\n",
    "        k_T = k.transpose(2, 3)\n",
    "\n",
    "        # calculate the attention weight \n",
    "        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n",
    "\n",
    "        # if there are any masks that needs to be applied\n",
    "        if mask is not None:\n",
    "            att_weight = att_weight.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # calculate the softmax \n",
    "        # att_weight shape: batch_size, head, seq_len_query, seq_len_key\n",
    "        att_weight = self.softmax(att_weight)\n",
    "\n",
    "        print(f'att_weight for first batch, first head, first query: {att_weight[0, 0, 0, :].sum()}')\n",
    "\n",
    "        # att_weight @ v shape: batch_size, head, seq_len_query, d_tensor\n",
    "        return att_weight @ v, att_weight\n",
    "\n",
    "# Define MultiHeadAttention Block \n",
    "class MultiHeadAttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    define multi head attention block using AttentionBlock module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_head): \n",
    "        \"\"\"\n",
    "        Multi-head self-attention utilize the parallelism of GPU\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param n_head: number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = AttentionBlock()\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # in the paper, d_v = d_k = d_q\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.Wconcat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split(self, tensor): \n",
    "        \"\"\"\n",
    "        split the tensor by number of head \n",
    "\n",
    "        :param tensor: tensor of shape batch_size  * seq_len * d_model\n",
    "        :return: return tensor of shape batch_size * n_head * seq_len * d_tensor\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = tensor.shape \n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "\n",
    "        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor \n",
    "\n",
    "    def concat(self, tensor): \n",
    "        \"\"\"\n",
    "        concat tensor. Inverse operation of split\n",
    "\n",
    "        :param tensor: tensor of shape batch_size * n_head * seq_len * d_tensor \n",
    "        :return: return tensor of shape batch_size * seq_len * d_model\n",
    "        \"\"\"\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.shape\n",
    "\n",
    "        d_model = n_head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return tensor \n",
    "    \n",
    "\n",
    "    def forward(self, q, k, v, mask=None): \n",
    "        # apply linear transformation to derive q, k, v \n",
    "        q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)\n",
    "\n",
    "        # split the tensor by number of heads\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        # apply attention to q, k, v \n",
    "        out, attn_weights = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # current attn_weights shape is batch_size * n_head * q_len * k_len\n",
    "        # mean it by dim 1\n",
    "        # eventually changing shape into batch_size * q_len * k_len\n",
    "        attn_weight = attn_weights.mean(dim=1)\n",
    "\n",
    "        # concat \n",
    "        out = self.concat(out)\n",
    "\n",
    "        # apply concat weight \n",
    "        out = self.Wconcat(out)\n",
    "        return out, attn_weight\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acdf03e0-8bb9-4194-9a59-a7ba2546f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define LayerNorm \n",
    "# class LayerNorm(nn.Module): \n",
    "#     \"\"\"\n",
    "#     Normalize all features for each samples. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, d_model, eps=1e-6): \n",
    "#         super(LayerNorm, self).__init__()\n",
    "#         self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "#         self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "#         self.eps = eps \n",
    "\n",
    "#     def forward(self, x): \n",
    "#         mean = x.mean(-1, keepdim=True)\n",
    "#         var = x.var(-1, keepdim=True, unbiased=False)\n",
    "\n",
    "#         out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         out = self.gamma * out + self.beta\n",
    "\n",
    "#         return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc7b94a-5680-4860-87df-a2665d85fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define FeedForward Network \n",
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1): \n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden) \n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1718d3-90ee-4c13-8363-76fd56515a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder Layer \n",
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm = nn.LayerNorm(d_model) \n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask): \n",
    "        residual = x \n",
    "        x = self.norm(x)\n",
    "        x, attn_weight = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "\n",
    "        x = self.dropout1(x) \n",
    "        x = self.norm(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x) \n",
    "\n",
    "        x =  self.dropout2(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x, attn_weight\n",
    "\n",
    "# Define Decoder Layer \n",
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask): \n",
    "        residual = dec\n",
    "        dec = self.norm1(dec)\n",
    "        \n",
    "        x, attn_weight1 = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm2(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x, attn_weight2 = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "        x = self.dropout2(x) \n",
    "        x = self.norm3(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x, attn_weight1, attn_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca21171-0527-4604-ae4b-38d4174ff317",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Encoder Model\n",
    "class Encoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = embedding\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, src_mask): \n",
    "        x = self.emb(x) \n",
    "        attn_weights = []\n",
    "        # get the mean of attention map batch_size * seq_len_src * seq_len_src \n",
    "        for layer in self.layers: \n",
    "            x, attn_weight = layer(x, src_mask)\n",
    "            attn_weights.append(attn_weight)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, torch.mean(torch.stack(attn_weights), dim=0)\n",
    "\n",
    "        \n",
    "class Decoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = embedding\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask): \n",
    "        trg = self.emb(trg)\n",
    "        attn_weights_1 = []\n",
    "        attn_weights_2 = []\n",
    "        for layer in self.layers: \n",
    "            trg, attn_weight_1, attn_weight_2 = layer(trg, enc_src, trg_mask, src_mask) \n",
    "            attn_weights_1.append(attn_weight_1)\n",
    "            attn_weights_2.append(attn_weight_2)\n",
    "\n",
    "        trg = self.norm(trg)\n",
    "        output = self.linear(trg)\n",
    "        return output, torch.mean(torch.stack(attn_weights_1), dim=0), torch.mean(torch.stack(attn_weights_2), dim=0)\n",
    "    \n",
    "\n",
    "# Define Transformer Model \n",
    "class Transformer(nn.Module): \n",
    "    \"\"\"\n",
    "    Transformer Model\n",
    "    \"\"\"\n",
    "    def __init__(self, src_pad_token, trg_pad_token, enc_voc_size, dec_voc_size, n_head, max_len, d_model, ffn_hidden, n_layers, drop_prob, device): \n",
    "        \"\"\"\n",
    "        Constructing Transformer Model \n",
    "\n",
    "        :param src_pad_token: embedding vector that represents <pad> in source \n",
    "        :param trg_pad_token: embedding vector that represents <pad> in target \n",
    "        :params enc_voc_size: number of vocabs that encoderEmbedder can handle\n",
    "        :params dec_voc_size: number of vocabs that decoderEmbedder can handle\n",
    "        :params ffn_hidden: hidden vector dimension for fastfeedforward layer \n",
    "        :params n_layers: number of EncoderLayer/DecoderLayer used\n",
    "        :params drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=dec_voc_size, drop_prob=drop_prob, device=device)\n",
    "        self.src_pad_token = src_pad_token\n",
    "        self.trg_pad_token = trg_pad_token\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(embedding=self.emb, d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, enc_voc_size=enc_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "        self.decoder = Decoder(embedding=self.emb, d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, dec_voc_size=dec_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "    \n",
    "    def make_src_mask(self, src): \n",
    "        # print(f'src: {src}')\n",
    "        # print(f'src_pad_token: {self.src_pad_token}')\n",
    "        # print(f'src != self.src_pad_token: {src != self.src_pad_token}')\n",
    "        src_mask = (src != self.src_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg): \n",
    "        trg_pad_mask = (trg != self.trg_pad_token).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        # make a look-ahead mask using torch.tril \n",
    "        # [[1 0 0]\n",
    "        #  [1 1 0]\n",
    "        #  [1 1 1]]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "        \n",
    "    \n",
    "    def forward(self, src, trg): \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src, enc_self_attn_weight = self.encoder(src, src_mask)\n",
    "        output, dec_self_attn_weight, enc_dec_attn_weight = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # current output shape is batch_size * dec_voc_size, which is desirable. \n",
    "        # We don't need to apply softmax because we are going to use CrossEntropyLoss as loss function\n",
    "        # which automatically applies log-softmax during calculation\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a6958-d332-4c2a-abc5-ff66dd794695",
   "metadata": {},
   "source": [
    "# Train the Model using datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a6a241-df71-4a97-bb02-1bc70f2fa2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ../config/config.ipynb\n",
    "%run ./data-loader.ipynb\n",
    "%run ./Transformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e61d776-cdbb-431d-a749-6198daf58c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "import torch \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a282067-acba-4324-8155-61e67650d5e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter #: 53940457\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model \n",
    "model = Transformer(\n",
    "    src_pad_token=src_pad_token, \n",
    "    trg_pad_token=trg_pad_token, \n",
    "    enc_voc_size=enc_voc_size, \n",
    "    dec_voc_size=dec_voc_size, \n",
    "    n_head=n_head, \n",
    "    max_len=max_len, \n",
    "    d_model=d_model, \n",
    "    ffn_hidden=ffn_hidden, \n",
    "    n_layers=n_layers, \n",
    "    drop_prob=drop_prob, \n",
    "    device=device).to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'model parameter #: {count_parameters(model)}')\n",
    "logger.info(f'model parameter #: {count_parameters(model)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e078fd4f-33e5-4e6c-b539-6f8f4c8f1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer \n",
    "optimizer = Adam(params=model.parameters(), lr=init_lr, weight_decay=weight_decay, eps=eps, betas=(0.9, 0.98))\n",
    "\n",
    "# Set Noam Scheduler\n",
    "scheduler = LRScheduler(optimizer, d_model, warmup_steps)\n",
    "# Setup loss function for training\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=src_pad_token, label_smoothing=0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a37978-5880-4075-a5ee-ee77404eb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store lr rate history per steps \n",
    "lr_history = []\n",
    "# store loss history per steps \n",
    "train_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d092cb-c592-4a28-8316-92e291ea493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_num): \n",
    "    model.train()\n",
    "    train_epoch_loss = 0 \n",
    "\n",
    "    for step, (kr_sentences, en_sentences) in tqdm(enumerate(train_dataloader)): \n",
    "\n",
    "        # tokenize kr_sentence \n",
    "        kr_tokenized = kr_tokenizer(kr_sentences, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        # tokenize en_sentence \n",
    "        # make en_sentence start with eos token(this is because current tokenizer don't have an sos token.)\n",
    "        en_sentences = ['</s> ' + s for s in en_sentences]\n",
    "        en_tokenized = en_tokenizer(en_sentences, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "\n",
    "        kr_tokenized = kr_tokenized.to(device)\n",
    "        en_tokenized = en_tokenized.to(device)\n",
    "\n",
    "        # out is the dec_voc_size vector \n",
    "        # during training, we exclude sep token \n",
    "\n",
    "        # remove eos token if the sentence is too long, and gets truncated.\n",
    "        # so we can prevent early-stopping(early-eos) \n",
    "        # out: batch_size * max_len * dec_voc_size\n",
    "        out = model(kr_tokenized, en_tokenized[:, :-1])\n",
    "\n",
    "        # remove sos token from en_tokenized when calculating loss because out will not include eos token in front of the sentence. \n",
    "        # en_tokenized: batch_size * (max_len-1)\n",
    "        en_tokenized = en_tokenized[:, 1:].to(device)\n",
    "\n",
    "        # out: batch_size * (max_len - 1) * dec_voc_size\n",
    "        out = out.permute(0, 2, 1).to(device)\n",
    "\n",
    "        loss = loss_func(out, en_tokenized)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(f'    EPOCH #{epoch_num} STEP #{step} | loss: {loss.item()}, avg_loss: {train_epoch_loss / (step + 1)}')\n",
    "            logger.info(f'    EPOCH #{epoch_num} STEP #{step} | loss: {loss.item()}, avg_loss: {train_epoch_loss / (step + 1)}')\n",
    "        \n",
    "        \n",
    "\n",
    "    train_step_loss = train_epoch_loss / (step+1)\n",
    "    # After training epoch, do evaluation \n",
    "\n",
    "    return train_step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4858b14-e7f6-40af-b412-01eb7653c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model \n",
    "def evaluate(): \n",
    "    model.eval()\n",
    "    test_epoch_loss = 0 \n",
    "    test_bleu_loss = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for step, (kr_sentences, en_sentences) in tqdm(enumerate(test_dataloader)): \n",
    "            # tokenize kr_sentence \n",
    "            kr_tokenized = kr_tokenizer(kr_sentences, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").input_ids\n",
    "            \n",
    "            # tokenize en_sentence \n",
    "            # make en_sentence start with eos token(this is because current tokenizer don't have an sos token.)\n",
    "            en_sentences = ['</s> ' + s for s in en_sentences]\n",
    "            en_tokenized = en_tokenizer(en_sentences, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            kr_tokenized = kr_tokenized.to(device)\n",
    "            en_tokenized = en_tokenized.to(device)\n",
    "\n",
    "\n",
    "            # this does not remove the eos token \n",
    "            # FIXME \n",
    "            out = model(kr_tokenized, en_tokenized[:, :-1])\n",
    "            \n",
    "\n",
    "            # remove sos token from en_tokenized when calculating loss because out will not include sos token. \n",
    "            en_tokenized = en_tokenized[:, 1:].to(device)\n",
    "    \n",
    "            out = out.permute(0, 2, 1).to(device)\n",
    "            \n",
    "            loss = loss_func(out, en_tokenized)\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "            # calcuate the bleu \n",
    "            # TODO\n",
    "        test_step_loss = test_epoch_loss / (step + 1)\n",
    "    return test_step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7080d85-a150-4e03-bb81-191a1bd882d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Start: current LR 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EPOCH #0 STEP #0 | loss: 11.198524475097656, avg_loss: 11.198524475097656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [10:51,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EPOCH #0 STEP #200 | loss: 6.86619234085083, avg_loss: 9.200099686485025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [10:56,  3.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Start: current LR \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Start: current LR \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m End: Train Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out, en_tokenized)\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:30\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:122\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    120\u001b[0m         clip_coef_clamped_device \u001b[38;5;241m=\u001b[39m clip_coef_clamped\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads:\n\u001b[0;32m--> 122\u001b[0m             \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch #{epoch} Start: current LR {optimizer.param_groups[0][\"lr\"]}')\n",
    "    logger.info(f'Epoch #{epoch} Start: current LR {optimizer.param_groups[0][\"lr\"]}')\n",
    "    \n",
    "    train_loss = train_epoch(epoch)\n",
    "    test_loss = evaluate()\n",
    "    lr_history.append(optimizer.param_groups[0][\"lr\"])\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    logger.info(f'Epoch #{epoch} End: Train Loss {train_loss}, Test Loss {test_loss}')\n",
    "\n",
    "    model_path = model_dir / f'model_{timestamp}_{epoch}' \n",
    "    torch.save(model.state_dict(), model_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20070a-81b3-4e09-a47d-7b553093587b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
