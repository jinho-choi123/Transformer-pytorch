{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab0035b-83a2-44f1-bb0a-100fc36d0be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ball/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd \n",
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88745aae-01d1-4fd9-8da3-cfba09d5e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS as device\n"
     ]
    }
   ],
   "source": [
    "# define device \n",
    "# configuration \n",
    "\n",
    "TOKENIZERS_PARALLELISM = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b657ad88-087b-4e22-b26d-b69b5a369c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "preprocessed_directory = preprocessed_directory = os.path.join(current_path, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b191bfd3-bedc-441e-a345-8eed33fbe22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import tokenizers\n",
    "kr_tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "en_tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff91b0d9-662c-45b5-bee2-f5ab77eca9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizers \n",
    "tmp_kr_sentence = \"오늘 하교길에 길고양이를 보았는데, 너무 귀여워서 집에 데려가고 싶었다. 하지만 그러지는 않았다.\"\n",
    "tmp_en_sentence = \"The cat I saw during heading home today was so cute, that I wanted to bring it to home.\"\n",
    "\n",
    "tmp_kr_tokenized = kr_tokenizer(tmp_kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "tmp_en_tokenized = en_tokenizer(tmp_en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=256, truncation=True)\n",
    "\n",
    "# print(kr_tokenizer.convert_ids_to_tokens(tmp_kr_tokenized.input_ids))\n",
    "# print(en_tokenizer.convert_ids_to_tokens(tmp_en_tokenized.input_ids))\n",
    "\n",
    "# print(kr_tokenizer.decode(tmp_kr_tokenized.input_ids, skip_special_tokens=True))\n",
    "\n",
    "# check if both tokenizer has pad token \n",
    "# print(kr_tokenizer.pad_token)\n",
    "# print(en_tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89ae1db-24b1-4c22-a532-a6bf8aa0d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(path=os.path.join(preprocessed_directory, \"train.parquet\"))\n",
    "df_test = pd.read_parquet(path=os.path.join(preprocessed_directory, \"test.parquet\"))\n",
    "df_validation = pd.read_parquet(path=os.path.join(preprocessed_directory, \"validation.parquet\"))\n",
    "\n",
    "\n",
    "\n",
    "class en2kr_Train_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_train\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "\n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        return kr_tokenized_ids, en_tokenized_ids\n",
    "        \n",
    "class en2kr_Test_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_test\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "        \n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        \n",
    "        return kr_tokenized_ids, en_tokenized_ids\n",
    "\n",
    "class en2kr_Validation_Dataset(Dataset): \n",
    "    def __init__(self, max_len): \n",
    "        self.data = df_validation\n",
    "        self.max_len = max_len \n",
    "        self.kr_tokenizer = kr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        row = self.data.iloc[[idx]]\n",
    "        en_sentence = row[\"english\"].item()\n",
    "        kr_sentence = row[\"korean\"].item()\n",
    "        kr_tokenized_ids = self.kr_tokenizer(kr_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "        en_tokenized_ids = self.en_tokenizer(en_sentence, add_special_tokens=True, padding=\"max_length\", max_length=self.max_len, truncation=True).input_ids\n",
    "\n",
    "        # kr_tokenized = self.kr_tokenizer.convert_ids_to_tokens(kr_tokenized_ids)\n",
    "        # en_tokenized = self.en_tokenizer.convert_ids_to_tokens(en_tokenized_ids)\n",
    "        \n",
    "        kr_tokenized_ids = torch.IntTensor(kr_tokenized_ids)\n",
    "        en_tokenized_ids = torch.IntTensor(en_tokenized_ids)\n",
    "        \n",
    "        return kr_tokenized_ids, en_tokenized_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb48b7b-bee2-4824-8984-45cd706d83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_dataset = en2kr_Train_Dataset(max_len=128)\n",
    "test_dataset = en2kr_Test_Dataset(max_len=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac0529-d529-46ce-a2dd-5f53c9e2859f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c95f00f-c1f3-422b-a701-56b8c8329eac",
   "metadata": {},
   "source": [
    "# Transformer Model Implementation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05dfc35a-d661-4bc4-a549-d1476c22fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import copy \n",
    "import math \n",
    "from torch.nn.functional import log_softmax\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b9255b-2fc1-4eb9-9b97-ab9a2723ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Token Embedding \n",
    "class TokenEmbeddings(nn.Embedding): \n",
    "    \"\"\"\n",
    "    Converting token into embedding vector\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        class for token embedding without positional encoding\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super(TokenEmbeddings, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "\n",
    "# Define Positional Encoding \n",
    "class PositionalEncoding(nn.Module): \n",
    "    \"\"\" \n",
    "    compute reusable sinusoid positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len, device): \n",
    "        \"\"\"\n",
    "        construct sinusoid positional encoding that is going to be reused everytime when it is needed\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # define a max_len * d_model size encoding matrix\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "\n",
    "        # since positional encoding is not learnable, we turn off the gradient engine\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        # define a position at the sequence\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        # expand the max_len vector to max_len * 1 matrix \n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        # define a sinusoid positional encoding\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "# Define Transformer Embedding \n",
    "class TransformerEmbedding(nn.Module): \n",
    "    \"\"\"\n",
    "    token embedding + positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device): \n",
    "        \"\"\"\n",
    "        initialize the embedding class for word+position embedding\n",
    "\n",
    "        :param vocab_size: number of vocabs that TokenEmbeddings can handle\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param max_len: maximum sequence length of token(a.k.a window size of attention method)\n",
    "        :param drop_prob: dropout probability to reduce overfitting\n",
    "        \"\"\"\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_emb = TokenEmbeddings(vocab_size, d_model)\n",
    "        self.position_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        tok_emb = self.token_emb(x)\n",
    "        pos_emb = self.position_emb(x)\n",
    "\n",
    "        return self.dropout(tok_emb+pos_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "082e2d7c-b5d7-463e-a65e-dbcea5e75acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Attention Block \n",
    "class AttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    compute scale dot product attention for Query, Key, Value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, eps=1e-12): \n",
    "        batch_size, head, length, d_tensor = k.shape\n",
    "\n",
    "        # calculate the k_T\n",
    "        k_T = k.transpose(2, 3)\n",
    "\n",
    "        # calculate the attention weight \n",
    "        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n",
    "\n",
    "        # if there are any masks that needs to be applied\n",
    "        if mask is not None:\n",
    "            att_weight = att_weight.masked_fill(mask == 0, -10000)\n",
    "\n",
    "        # calculate the softmax \n",
    "        # att_weight shape: batch_size, head, seq_len, seq_len\n",
    "        att_weight = self.softmax(att_weight)\n",
    "\n",
    "        # att_weight @ v shape: batch_size, head, seq_len, d_model\n",
    "        return att_weight @ v, att_weight\n",
    "\n",
    "# Define MultiHeadAttention Block \n",
    "class MultiHeadAttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    define multi head attention block using AttentionBlock module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_head): \n",
    "        \"\"\"\n",
    "        Multi-head self-attention utilize the parallelism of GPU\n",
    "\n",
    "        :param d_model: dimension of embedding vector\n",
    "        :param n_head: number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = AttentionBlock()\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # in the paper, d_v = d_k = d_q\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.Wconcat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split(self, tensor): \n",
    "        \"\"\"\n",
    "        split the tensor by number of head \n",
    "\n",
    "        :param tensor: tensor of shape batch_size  * seq_len * d_model\n",
    "        :return: return tensor of shape batch_size * n_head * seq_len * d_tensor\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = tensor.shape \n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "\n",
    "        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor \n",
    "\n",
    "    def concat(self, tensor): \n",
    "        \"\"\"\n",
    "        concat tensor. Inverse operation of split\n",
    "\n",
    "        :param tensor: tensor of shape batch_size * n_head * seq_len * d_tensor \n",
    "        :return: return tensor of shape batch_size * seq_len * d_model\n",
    "        \"\"\"\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.shape\n",
    "\n",
    "        d_model = n_head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return tensor \n",
    "    \n",
    "\n",
    "    def forward(self, q, k, v, mask=None): \n",
    "        # apply linear transformation to derive q, k, v \n",
    "        q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)\n",
    "\n",
    "        # split the tensor by number of heads\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        # apply attention to q, k, v \n",
    "        out, att_weight = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # concat \n",
    "        out = self.concat(out)\n",
    "\n",
    "        # apply concat weight \n",
    "        out = self.Wconcat(out)\n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acdf03e0-8bb9-4194-9a59-a7ba2546f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LayerNorm \n",
    "class LayerNorm(nn.Module): \n",
    "    \"\"\"\n",
    "    Normalize all features for each samples. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-12): \n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, x): \n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc7b94a-5680-4860-87df-a2665d85fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define FeedForward Network \n",
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1): \n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden) \n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e1718d3-90ee-4c13-8363-76fd56515a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder Layer \n",
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm1 = LayerNorm(d_model) \n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask): \n",
    "        residual = x \n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "\n",
    "        x = self.dropout1(x) \n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x) \n",
    "\n",
    "        x =  self.dropout2(x)\n",
    "        x = self.norm2(x + residual)\n",
    "\n",
    "        return x \n",
    "\n",
    "# Define Decoder Layer \n",
    "class DecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob): \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttentionBlock(d_model, n_head)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = FeedForwardBlock(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask): \n",
    "        residual = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        if enc is not None: \n",
    "            residual = x \n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "            x = self.dropout2(x) \n",
    "            x = self.norm2(x + residual)\n",
    "\n",
    "        residual = x \n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + residual)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca21171-0527-4604-ae4b-38d4174ff317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder Model\n",
    "class Encoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=enc_voc_size, drop_prob=drop_prob, device=device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask): \n",
    "        x = self.emb(x) \n",
    "        for layer in self.layers: \n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "        \n",
    "class Decoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder for Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device): \n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model, max_len=max_len, vocab_size=dec_voc_size, drop_prob=drop_prob, device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask): \n",
    "        trg = self.emb(trg)\n",
    "        for layer in self.layers: \n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask) \n",
    "\n",
    "        output = self.linear(trg)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Define Transformer Model \n",
    "class Transformer(nn.Module): \n",
    "    \"\"\"\n",
    "    Transformer Model\n",
    "    \"\"\"\n",
    "    def __init__(self, src_pad_token, trg_pad_token, trg_sos_token, enc_voc_size, dec_voc_size, ffn_hidden, n_layers, drop_prob, device): \n",
    "        \"\"\"\n",
    "        Constructing Transformer Model \n",
    "\n",
    "        :param src_pad_token: embedding vector that represents <pad> in source \n",
    "        :param trg_pad_token: embedding vector that represents <pad> in target \n",
    "        :param trg_sos_token: embedding vector that represents <sos> in target \n",
    "        :params enc_voc_size: number of vocabs that encoderEmbedder can handle\n",
    "        :params dec_voc_size: number of vocabs that decoderEmbedder can handle\n",
    "        :params ffn_hidden: hidden vector dimension for fastfeedforward layer \n",
    "        :params n_layers: number of EncoderLayer/DecoderLayer used\n",
    "        :params drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.src_pad_token = src_pad_token\n",
    "        self.trg_pad_token = trg_pad_token\n",
    "        self.trg_sos_token = trg_sos_token\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, enc_voc_size=enc_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "        self.decoder = Decoder(d_model=d_model, n_head=n_head, max_len=max_len, ffn_hidden=ffn_hidden, dec_voc_size=dec_voc_size, drop_prob=drop_prob, n_layers=n_layers, device=device)\n",
    "\n",
    "    def make_src_mask(self, src): \n",
    "        # print(f'src: {src}')\n",
    "        # print(f'src_pad_token: {self.src_pad_token}')\n",
    "        # print(f'src != self.src_pad_token: {src != self.src_pad_token}')\n",
    "        src_mask = (src != self.src_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg): \n",
    "        trg_pad_mask = (trg != self.trg_pad_token).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        # make a look-ahead mask using torch.tril \n",
    "        # [[1 0 0]\n",
    "        #  [1 1 0]\n",
    "        #  [1 1 1]]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "        \n",
    "    \n",
    "    def forward(self, src, trg): \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d49aa-5968-4b95-a9ed-f7f12967a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fe859-957f-4306-bd5b-ede75622a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b0bf0e-3048-4adf-b3a2-6f63d15e481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from collections import Counter \n",
    "import numpy \n",
    "\n",
    "# compute the statistics for BLEU \n",
    "def bleu_stats(hypothesis, reference): \n",
    "    stats = [] \n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "\n",
    "    for n in range(1, 5): \n",
    "        s_ngrams = Counter(\n",
    "            [tuple(hypothesis[i:i+n]) for i in range(len(hypothesis) + 1 - n)]\n",
    "        )\n",
    "\n",
    "        r_ngrams = Counter(\n",
    "            [tuple(reference[i:i+n]) for i in range(len(reference) + 1 - n)]\n",
    "        )\n",
    "\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
    "\n",
    "    return stats \n",
    "\n",
    "def bleu(stats): \n",
    "    for i in stats:\n",
    "        if i == 0:\n",
    "            return 0 \n",
    "\n",
    "    (h_len, r_len) = stats[:2]\n",
    "    log_bleu_prec = sum(\n",
    "        [math.log(float(x)/y) for x, y in zip(stats[2::2], stats[3::2])]\n",
    "    ) / 4.\n",
    "\n",
    "    return math.exp(min([0, 1 - float(r_len) / h_len]) + log_bleu_prec)\n",
    "\n",
    "def get_bleu(hypotheses, reference):\n",
    "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return 100 * bleu(stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a6958-d332-4c2a-abc5-ff66dd794695",
   "metadata": {},
   "source": [
    "# Train the Model using datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e61d776-cdbb-431d-a749-6198daf58c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "import torch \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b840cd15-ff96-458a-b4f9-a691a205a839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS as device\n"
     ]
    }
   ],
   "source": [
    "# define device \n",
    "# configuration \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df4d68c3-d09b-4801-87b9-930d44de4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some configuration of training \n",
    "d_model = 256 \n",
    "n_head = 1\n",
    "max_len = 128\n",
    "ffn_hidden = 64 \n",
    "n_layers=2 \n",
    "drop_prob=0.1\n",
    "epochs=300\n",
    "init_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5afc06aa-b123-4487-be36-eafafebf214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS as device\n"
     ]
    }
   ],
   "source": [
    "%run data-loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75157154-400d-4758-8522-07db3f257311",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Transformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5014b77-d49d-4596-aba6-8c04f43b6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run BLEU-metric.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa0d6087-fc7d-4bfb-ba43-10b5b802b51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_pad_token: 0\n",
      "trg_pad_token: 0\n",
      "trg_sos_token: 102\n",
      "enc_voc_size: 42000\n",
      "dec_voc_size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Define some configuration of training \n",
    "\n",
    "src_pad_token = kr_tokenizer.pad_token_id\n",
    "trg_pad_token = en_tokenizer.pad_token_id\n",
    "trg_sos_token = en_tokenizer.sep_token_id\n",
    "enc_voc_size = kr_tokenizer.vocab_size\n",
    "dec_voc_size = en_tokenizer.vocab_size\n",
    "\n",
    "print(f'src_pad_token: {src_pad_token}')\n",
    "print(f'trg_pad_token: {trg_pad_token}')\n",
    "print(f'trg_sos_token: {trg_sos_token}')\n",
    "print(f'enc_voc_size: {enc_voc_size}')\n",
    "print(f'dec_voc_size: {dec_voc_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a282067-acba-4324-8155-61e67650d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter #: 28126266\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model \n",
    "model = Transformer(src_pad_token, trg_pad_token, trg_sos_token, enc_voc_size, dec_voc_size, ffn_hidden, n_layers, drop_prob, device).to(device)\n",
    "model.train()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'model parameter #: {count_parameters(model)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e078fd4f-33e5-4e6c-b539-6f8f4c8f1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer \n",
    "optimizer = Adam(params=model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=src_pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42d092cb-c592-4a28-8316-92e291ea493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_num): \n",
    "    train_epoch_loss = 0 \n",
    "\n",
    "    for step, (kr_tokenized, en_tokenized) in tqdm(enumerate(train_dataloader)): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kr_tokenized = kr_tokenized.to(device)\n",
    "        en_tokenized = en_tokenized.to(device)\n",
    "\n",
    "        out = model(kr_tokenized, en_tokenized[:, :-1])\n",
    "\n",
    "        # remove sos token from en_tokenized when calculating loss because out will not include sos token. \n",
    "        en_tokenized = en_tokenized[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        out = out.contiguous().view(-1, out.shape[-1])\n",
    "\n",
    "        loss = loss_func(out, en_tokenized.type(torch.LongTensor))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "    train_step_loss = train_epoch_loss / (step+1)\n",
    "    # After training epoch, do evaluation \n",
    "\n",
    "    return train_step_loss\n",
    "    \n",
    "\n",
    "# evaluate the model \n",
    "def evaluate(): \n",
    "    model.eval()\n",
    "    test_epoch_loss = 0 \n",
    "    test_bleu_loss = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for step, batch in tqdm(enumerate(test_dataloader)): \n",
    "            kr_tokenized = batch[\"kr\"].to(device)\n",
    "            en_tokenized = batch[\"en\"].to(device)\n",
    "\n",
    "            out = model(kr_tokenized, en_tokenized[:, :-1])\n",
    "\n",
    "            # remove sos token from en_tokenized when calculating loss because out will not include sos token. \n",
    "            en_tokenized = en_tokenized[:, 1:].contiguous().view(-1)\n",
    "    \n",
    "            out = out.contiguous().view(-1, out.shape[-1])\n",
    "            loss = loss_func(out, en_tokenized.type(torch.LongTensor))\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "            # calcuate the bleu \n",
    "            # TODO\n",
    "    return test_step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7080d85-a150-4e03-bb81-191a1bd882d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0it [00:42, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 13.41 GB, other allocations: 64.78 MB, max allowed: 18.13 GB). Tried to allocate 7.39 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m      7\u001b[0m     best_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100_000_000\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     13\u001b[0m en_tokenized \u001b[38;5;241m=\u001b[39m en_tokenized[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men_tokenized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3465\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3399\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[1;32m   3400\u001b[0m \n\u001b[1;32m   3401\u001b[0m \u001b[38;5;124;03mSee :class:`~torch.nn.CrossEntropyLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3462\u001b[0m \u001b[38;5;124;03m    >>> loss.backward()\u001b[39;00m\n\u001b[1;32m   3463\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight):\n\u001b[0;32m-> 3465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3467\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3471\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_average\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/overrides.py:1717\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1717\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/transformer-tutorial/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 13.41 GB, other allocations: 64.78 MB, max allowed: 18.13 GB). Tried to allocate 7.39 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(epoch)\n",
    "    test_loss = evaluate()\n",
    "\n",
    "    best_vloss = 100_000_000\n",
    "\n",
    "    print(f'Epoch {epoch}: Train Loss {train_loss}, Test Loss {test_loss}')\n",
    "\n",
    "    if test_loss < best_vloss:\n",
    "        best_vloss = avg_vloss \n",
    "        model_path = f'models/model_{timestamp}_{epoch}' \n",
    "        torch.save(model.state_dict(), model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20070a-81b3-4e09-a47d-7b553093587b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
